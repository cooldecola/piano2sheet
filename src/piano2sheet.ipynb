{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "import mahotas       # Otsu thresholding\n",
    "import bisect        # Key insert point\n",
    "import imutils       # Crop and resize images\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "import os            # Folder paths\n",
    "import sys           # Exit function\n",
    "import glob          # Folder searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note class\n",
    "class Note: \n",
    "    def __init__(self, centroid_x, y_dot):\n",
    "        self.centroid_x = centroid_x\n",
    "        self.y_dot = y_dot\n",
    "\n",
    "# just  a function for printing images\n",
    "def display_img(title, img):\n",
    "    cv2.imshow(title, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "      \n",
    "# function for getting next note\n",
    "def getNextNote(first_note):\n",
    "    if \"#\" in first_note: \n",
    "        octave = first_note[2]\n",
    "        if first_note[:2] == \"A#\":\n",
    "            return (\"C#\" + octave)\n",
    "        elif first_note[:2] == \"C#\":\n",
    "            return (\"D#\" + octave)\n",
    "        elif first_note[:2] == \"D#\":\n",
    "            return (\"F#\" + octave)\n",
    "        elif first_note[:2] == \"F#\":\n",
    "            return (\"G#\" + octave)\n",
    "        elif first_note[:2] == \"G#\":\n",
    "            next_octave = int(octave) + 1\n",
    "            return (\"A#\" + str(next_octave))\n",
    "    \n",
    "    else: \n",
    "        octave = first_note[1]\n",
    "        if first_note[0] == \"A\":\n",
    "            return (\"B\" + octave)\n",
    "        elif first_note[0] == \"B\":\n",
    "            return (\"C\" + octave)\n",
    "        elif first_note[0] == \"C\":\n",
    "            return (\"D\" + octave)\n",
    "        elif first_note[0] == \"D\":\n",
    "            return (\"E\" + octave)\n",
    "        elif first_note[0] == \"E\":\n",
    "            return (\"F\" + octave)\n",
    "        elif first_note[0] == \"F\":\n",
    "            return (\"G\" + octave)\n",
    "        elif first_note[0] == \"G\":\n",
    "            next_octave = int(octave) + 1\n",
    "            return (\"A\" + str(next_octave))\n",
    "\n",
    "# Function for Gaussian Blurring\n",
    "def gaussianBlurring(gray_img, blur_sq, std_dev = 0):\n",
    "    return cv2.GaussianBlur(gray_img, (blur_sq, blur_sq), std_dev)\n",
    "\n",
    "# Function for Canny Edge Detection\n",
    "def cannyDetection(blurred_img, th1, th2, apertureSize = 3):\n",
    "    return cv2.Canny(blurred_img, th1, th2, apertureSize)\n",
    "\n",
    "# Function to return the top and bottom of the keyboard using HoughLines\n",
    "def keyboardYCoords(edged_img, rho = 1, theta = np.pi/180, threshold = None):\n",
    "    \n",
    "    if threshold is None:\n",
    "        threshold = edged_img.shape[1]//2 # half the width\n",
    "    \n",
    "    lines = cv2.HoughLines(edged_img, rho, theta, threshold) \n",
    "    y_cord = [] #the y-value of the lines generated from hough transform\n",
    "\n",
    "    #iterating through lines\n",
    "    for line in lines: \n",
    "        rho_l, theta_l = line[0]\n",
    "        a = np.cos(theta_l)\n",
    "        b = np.sin(theta_l)\n",
    "        x0 = a * rho_l\n",
    "        y0 = b * rho_l\n",
    "        y_cord.append(y0) #appending to list\n",
    "\n",
    "    y_cord.sort(reverse=True)\n",
    "    return y_cord[0:2]\n",
    "\n",
    "# Function to threshold an image\n",
    "def theshold(gray_img, th1 = 90, th2 = 150, thresh_type = cv2.THRESH_BINARY_INV):\n",
    "    _, threshed_img = cv2.threshold(gray_img, th1, th2, thresh_type)\n",
    "    return threshed_img\n",
    "\n",
    "\n",
    "        \n",
    "# Function for doing connected components\n",
    "def connectedComponents(binarized_img, img, display_result):\n",
    "    connectivity = 1\n",
    "    output = cv2.connectedComponentsWithStats(binarized_img, connectivity, cv2.CV_32S)\n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "\n",
    "    final_labels = []\n",
    "\n",
    "    output = img.copy()\n",
    "\n",
    "    for i in range(1, num_labels):\n",
    "        x = stats[i, cv2.CC_STAT_LEFT]\n",
    "        y = stats[i, cv2.CC_STAT_TOP]\n",
    "        w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "        h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        (cX, cY) = centroids[i]\n",
    "        \n",
    "        if (100 < area < np.inf):\n",
    "            final_labels.append([i,cX])\n",
    "            \n",
    "            if (display_result):\n",
    "                cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                cv2.circle(output, (int(cX), int(cY)), 4, (255,255,0), -1)\n",
    "                componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "                cv2.imshow(\"Output\", output)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    key_width = statistics.median(stats[:, cv2.CC_STAT_WIDTH])\n",
    "    cv2.destroyAllWindows()\n",
    "    return final_labels, key_width\n",
    "\n",
    "\n",
    "def displayCentroid(key_list, img):\n",
    "    y = img.shape[0]*3//4\n",
    "    for (note, centroid) in key_list: \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        line = cv2.line(img,(int(centroid),0),(int(centroid),900),(0,0,255),1)\n",
    "        text_label = cv2.putText(img, note, (int(centroid), y), font, 0.5, (0,255,0), 1)\n",
    "        cv2.imshow(\"Key Label\", img)\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def key_pressed(key_list, key_index):\n",
    "    insertion_point = bisect.bisect_left(key_list[:,1].astype(float),key_index)\n",
    "    \n",
    "    #Insertion outside our index, means to insert it at the end (return the last key)\n",
    "    if insertion_point >= len(key_list):\n",
    "        insertion_point = len(key_list)-1\n",
    "#     print(insertion_point)\n",
    "#     print('You pressed the {} key.'.format(key_list[insertion_point,0]))\n",
    "\n",
    "    note = key_list[insertion_point,0]\n",
    "    index = insertion_point\n",
    "\n",
    "    return note, index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # For downloading YouTube videos\n",
    "# def my_hook(d):\n",
    "#     if d['status'] == 'finished':\n",
    "#         print('Download complete.')\n",
    "#     elif d['status'] == 'error':\n",
    "#         print('Error in downloading file - exiting program!')\n",
    "#         sys.exit()\n",
    "        \n",
    "        \n",
    "# # Function to download a YouTube video\n",
    "# def downloadYouTube(videourl, path = './videos/video_to_process.%(ext)s', quiet = True):\n",
    "    \n",
    "#     ydl_opts = {'outtmpl': path,\n",
    "#                'quiet': quiet,\n",
    "#                'progress_hooks': [my_hook]}\n",
    "#     try:\n",
    "#         with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "#             ydl.download([videourl])\n",
    "#     except youtube_dl.utils.DownloadError:\n",
    "#         print('Exiting program!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ask user for YouTube link\n",
    "- Download (?) video (or atleast a temporary copy)\n",
    "- Using the first few frames, detect the keyboard\n",
    "    - If 72 keys are not detected, try next few frames and repeat\n",
    "    - If they can't be detected at all, end program\n",
    "- Label black and white keys\n",
    "- Run algorithm, get array with start/end times for each note\n",
    "- Convert to .midi\n",
    "- Convert to sheet music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on a 'Piano' class - initialized with an image.\n",
    "Methods:\n",
    "-Gray keyboard\n",
    "-Blur keyboard\n",
    "-Canny Keyboard, etc. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyboard: \n",
    "    def __init__(self, youtubeURL):\n",
    "        self.youtubeURL = youtubeURL\n",
    "        \n",
    "    # For downloading YouTube videos\n",
    "    def my_hook(self, d):\n",
    "        if d['status'] == 'finished':\n",
    "            print('Download complete.')\n",
    "        elif d['status'] == 'error':\n",
    "            print('Error in downloading file - exiting program!')\n",
    "            sys.exit()\n",
    "            \n",
    "    # Remove previously downloaded file\n",
    "    def clear_previous(self, path = \"./videos/video_to_process*\"):\n",
    "        vtp = glob.glob(path)\n",
    "        if vtp:\n",
    "            camera = cv2.VideoCapture(vtp[0])\n",
    "            camera.release()\n",
    "            os.remove(vtp[0])\n",
    "\n",
    "    # Function to download a YouTube video\n",
    "    def downloadYouTube(self, path = './videos/video_to_process.%(ext)s', quiet = True):\n",
    "    \n",
    "        self.clear_previous()\n",
    "    \n",
    "        ydl_opts = {'outtmpl': path,\n",
    "                   'quiet': quiet,\n",
    "                   'progress_hooks': [self.my_hook]}\n",
    "        try:\n",
    "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "                print('Downloading video...')\n",
    "                ydl.download([self.youtubeURL])\n",
    "        except youtube_dl.utils.DownloadError:\n",
    "            print('Exiting program!')\n",
    "            \n",
    "    # Use the file called video_to_process and detect our 88 keys\n",
    "    def detect_keys(self, resize_width, bl_blur_sq, bl_canny_th1, bl_canny_th2, bl_thresh1, bl_thresh2,\n",
    "                wh_blur_sq, path = \"./videos/video_to_process*\",num_bl_keys = 36, num_wh_keys = 52):\n",
    "        vtp = glob.glob(path)\n",
    "        if vtp:\n",
    "            camera = cv2.VideoCapture(vtp[0])\n",
    "            # Could not access the file\n",
    "            if not camera.isOpened():\n",
    "                print('Error - video file could not be read!')\n",
    "                sys.exit()\n",
    "            # Process video\n",
    "            else:\n",
    "                print('Detecting keys...')\n",
    "                while True:\n",
    "                    #grabbed is a boolean than tells us if there is a valid frame\n",
    "                    (grabbed, frame) = camera.read()\n",
    "                    if not grabbed:\n",
    "                        break\n",
    "\n",
    "                    frame = imutils.resize(frame,width = resize_width)\n",
    "\n",
    "                    # Get the bottom-half of the frame (where the keyboard lies) and process from here\n",
    "                    keys = frame[frame.shape[0]//2:,:]\n",
    "                    gray_keys = cv2.cvtColor(keys, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # Process\n",
    "                    blurred = gaussianBlurring(gray_keys, blur_sq = bl_blur_sq)\n",
    "                    edges = cannyDetection(blurred, th1 = bl_canny_th1, th2 = bl_canny_th2)\n",
    "\n",
    "                    # Crop\n",
    "                    crop_coordinates  = keyboardYCoords(edges)\n",
    "                    cropped_keys      = keys[int(crop_coordinates[1])+20:int(crop_coordinates[0])]\n",
    "                    cropped_gray_keys = gray_keys[int(crop_coordinates[1])+20:int(crop_coordinates[0])]\n",
    "\n",
    "                    # Labels keys\n",
    "                    thresh_keys = theshold(cropped_gray_keys, th1 = bl_thresh1, th2 = bl_thresh2)\n",
    "\n",
    "                    # Black keys\n",
    "                    final_labels_bl, key_width_bl = connectedComponents(thresh_keys, cropped_keys, False)\n",
    "                    if len(final_labels_bl) == num_bl_keys: \n",
    "                        first_note = \"A#0\"\n",
    "                        for i in range(num_bl_keys):\n",
    "                            final_labels_bl[i][0] = first_note\n",
    "                            first_note = getNextNote(first_note)\n",
    "\n",
    "                        final_labels_bl = sorted(final_labels_bl, key=lambda x: x[1])\n",
    "\n",
    "                    # White keys\n",
    "                    blurred_w = gaussianBlurring(cropped_gray_keys, blur_sq = wh_blur_sq)\n",
    "                    T = mahotas.thresholding.otsu(blurred_w)*1.3\n",
    "                    thresh_keys_w = cropped_gray_keys.copy()\n",
    "                    thresh_keys_w[thresh_keys_w>T] = 255\n",
    "                    thresh_keys_w[thresh_keys_w<T] = 0\n",
    "                    final_labels_w, key_width_w = connectedComponents(thresh_keys_w, cropped_keys, False)\n",
    "\n",
    "                    if len(final_labels_w) == num_wh_keys: \n",
    "                        first_note = \"A0\"\n",
    "                        for j in range(num_wh_keys):\n",
    "                            final_labels_w[j][0] = first_note\n",
    "                            first_note = getNextNote(first_note)\n",
    "\n",
    "                        final_labels_w = sorted(final_labels_w, key=lambda x: x[1])\n",
    "\n",
    "                    # Determine if they sum to 88 keys (36 black, 52 white) - if not, try next frame\n",
    "                    if len(final_labels_bl) == num_bl_keys and len(final_labels_w) == num_wh_keys:\n",
    "                        self.black_keys = final_labels_bl\n",
    "                        self.white_keys = final_labels_w\n",
    "                        self.black_width = key_width_bl\n",
    "                        self.white_width = key_width_w\n",
    "                        self.keyboard_img = frame\n",
    "                        camera.release()         # Release cv2 camera object\n",
    "                        cv2.destroyAllWindows()  # Destroy any cv2 windows\n",
    "                        print('Key detection complete.')\n",
    "                        break\n",
    "\n",
    "                #     #Show the frame + drawn rectangle\n",
    "                #     cv2.imshow(\"Face\", thresh_keys)\n",
    "\n",
    "    #                 #Can break early by pressing \"q\"\n",
    "    #                 if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #                     break\n",
    "\n",
    "            # If at this point we've looped through everything and we don't have 72 keys\n",
    "            camera.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            if len(final_labels_bl) != 36 and len(final_labels_w) != 52:\n",
    "                print('Error in processing file - exiting program!')\n",
    "                sys.exit()\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Glob did not find a valid file\n",
    "        else:\n",
    "            print('Error - video path could not be accessed!')\n",
    "            sys.exit()\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Assign end-of-range for each key\n",
    "    def key_ranges(self):\n",
    "\n",
    "        # Array\n",
    "        full_key_list = self.black_keys + self.white_keys\n",
    "        full_key_list = sorted(full_key_list, key=lambda x: x[1].astype(float))\n",
    "        full_key_list = np.array(full_key_list)\n",
    "\n",
    "        # Empty list\n",
    "        tmp_list = np.empty([len(full_key_list), 2], dtype='object')\n",
    "\n",
    "        # Loop through and assign end-of-range for each key\n",
    "        # CASE: white key adjacent to black key: end of the white key range is the adjacent black key's centroid - black/2\n",
    "        #       black key:                       end of the black key range is the black key's centroid + black/2\n",
    "        #       white key adjacent to white key: end of the white key range is the half-way point between the adjacent centroids\n",
    "        for i in range(0,len(full_key_list)-1):\n",
    "            if len(full_key_list[i,0])==1 and len(full_key_list[i+1,0])>1: # White adjacent to black\n",
    "                tmp_list[i,1] = full_key_list[i+1,1].astype(float) - self.black_width/2\n",
    "            elif len(full_key_list[i,0])>1: # Black key\n",
    "                tmp_list[i,1] = full_key_list[i,1].astype(float) + self.black_width/2\n",
    "            else: # White key adjacent to white key\n",
    "                tmp_list[i,1] = (full_key_list[i,1].astype(float)+ full_key_list[i+1,1].astype(float))/2\n",
    "\n",
    "            # No change to the actual note (only the distances, above) for the first key\n",
    "            tmp_list[i,0] = full_key_list[i,0]\n",
    "\n",
    "        #For the last key, just take it to infinity\n",
    "        tmp_list[-1,1] = np.inf\n",
    "        tmp_list[-1,0] = full_key_list[-1,0]\n",
    "\n",
    "        full_key_list = tmp_list\n",
    "\n",
    "        self.keyboard_array = full_key_list\n",
    "            \n",
    "    \n",
    "    # Return the centroid of the black/white keys alongside the median width of each\n",
    "    def getKeys(self):\n",
    "        return [self.black_keys, self.white_keys, self.black_width, self.white_width]\n",
    "    \n",
    "    # Return the frame that 88 keys were successfully identified from\n",
    "    def getFrame(self):\n",
    "        return self.keyboard_img\n",
    "    \n",
    "    def getFullKeyList(self):\n",
    "        return self.keyboard_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting keys...\n",
      "Key detection complete.\n"
     ]
    }
   ],
   "source": [
    "#Download a YouTube video and process it to determine where the centroid of the black and white keys are\n",
    "keyboard = Keyboard('https://www.youtube.com/watch?v=skFugVOqBM4')\n",
    "# keyboard.downloadYouTube()\n",
    "# keyboard.detect_keys(resize_width = 600, bl_blur_sq = 5, bl_canny_th1 = 200, bl_canny_th2 = 200, \n",
    "#                     bl_thresh1 = 90, bl_thresh2 = 150, wh_blur_sq = 7)\n",
    "# keyboard.key_ranges()\n",
    "\n",
    "# [black, white, black_width, white_width] = keyboard.getKeys()\n",
    "# key_img = keyboard.getFrame()\n",
    "# keyboard_array = keyboard.getFullKeyList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "keyboard_array\n",
    "#Find the appropriate index for any given centroid via:\n",
    "# note, index = key_pressed(full_key_list, 999)\n",
    "# print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to delete the processed video\n",
    "# keyboard.clear_previous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE BELOW THIS LINE ##########################\n",
    "___________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load image in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start at the first frame - before ANY keys are clicked. This is because if a note is coloured on the keys themselves, our thresholding set-up doesn't work.\n",
    "\n",
    "**There is an issue when people have intros...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #reading in image\n",
    "# keys = cv2.imread(\"images\\synthesia.png\")\n",
    "# keys = keys[keys.shape[0]//2:,:]\n",
    "\n",
    "# #grayscale\n",
    "# gray_keys = cv2.cvtColor(keys, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ################################\n",
    "# ################# edge detection\n",
    "# std_dev = 0\n",
    "# k = 5\n",
    "# t1 = 200\n",
    "# t2 = 200\n",
    "# blurred = cv2.GaussianBlur(gray_keys, (k,k), std_dev)\n",
    "# edges = cv2.Canny(blurred, t1,t2, apertureSize = 3)\n",
    "\n",
    "# ################################\n",
    "# ################# hough transform \n",
    "# lines = cv2.HoughLines(edges, 1, np.pi/180, 300) \n",
    "# y_cord = [] #the y-value of the lines generated from hough transform\n",
    "\n",
    "# #iterating through lines\n",
    "# for line in lines: \n",
    "#     rho, theta = line[0]\n",
    "#     a = np.cos(theta)\n",
    "#     b = np.sin(theta)\n",
    "#     x0 = a * rho\n",
    "#     y0 = b * rho\n",
    "#     y_cord.append(y0) #appending to list\n",
    "\n",
    "# print(y_cord)\n",
    "\n",
    "# #Only want to two lines from hough transform that crops out image of piano\n",
    "# if (len(y_cord) > 2):\n",
    "#     y_cord.sort(reverse=True)\n",
    "#     y_cord.pop()\n",
    "    \n",
    "    \n",
    "\n",
    "# #crop gray\n",
    "# cropped_keys      = keys[int(y_cord[1])+20:int(y_cord[0])]\n",
    "# cropped_gray_keys = gray_keys[int(y_cord[1])+20:int(y_cord[0])]\n",
    "\n",
    "# ################################\n",
    "# ################# thresholding\n",
    "# _, th1 = cv2.threshold(cropped_gray_keys, 90, 150, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# plt.imshow(edges)\n",
    "# # plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(th1, cmap = \"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Label Black Keys\n",
    "We don't need to do distance math anymore. If we detect 36 black keys, we know that the first black key is A#."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*final_labels* is the 36 integer indices that tell us if the returned centroids are black keys. \n",
    "\n",
    "For example, if centroids returns 39 possible connected components, final_label is the list of 36 indices that we consider are black keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get the average width of a black key\n",
    "### Also get all balck key info (centroid etc)\n",
    "This will be used to determine the discrete range of each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ################################\n",
    "# ################# Connected Components (for black keys)\n",
    "# #SWITCH TRUE TO FALSE IF YOU DONT WANT TO SHOW OUTPUT\n",
    "# final_labels_bl, key_width_bl = connectedComponents(th1, cropped_keys, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ################################\n",
    "# ################# Labelling (for black keys)\n",
    "# if len(final_labels_bl) == 36: \n",
    "#     first_note = \"A#0\"\n",
    "#     for i in range(36):\n",
    "#         final_labels_bl[i][0] = first_note\n",
    "#         first_note = getNextNote(first_note)\n",
    "\n",
    "# final_labels_bl = sorted(final_labels_bl, key=lambda x: x[1])\n",
    "\n",
    "# #just displaying to test\n",
    "# # displayCentroid(final_labels_bl, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Label White Keys\n",
    "Testing white keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# ################# Connected Components (for white keys)\n",
    "# k = 7\n",
    "# blurred = cv2.GaussianBlur(cropped_gray_keys, (k,k), 0)\n",
    "# T = mahotas.thresholding.otsu(blurred)*1.3\n",
    "# th2 = cropped_gray_keys.copy()\n",
    "# th2[th2>T] = 255\n",
    "# th2[th2<T] = 0\n",
    "# final_labels_w, key_width_w = connectedComponents(th2, cropped_keys, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# ################# Labelling (for white keys)\n",
    "# if len(final_labels_w) == 52: \n",
    "#     first_note = \"A0\"\n",
    "#     for i in range(52):\n",
    "#         final_labels_w[i][0] = first_note\n",
    "#         first_note = getNextNote(first_note)\n",
    "\n",
    "# final_labels_w = sorted(final_labels_w, key=lambda x: x[1])\n",
    "\n",
    "# #just displaying to test\n",
    "# # displayCentroid(final_labels_w, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Assign Ranges\n",
    "Order both the white and black keys together. \n",
    "\n",
    "For the range x:0 -> end, we assign a specify range to each key. For example, A: 0 - 10, A#: 10 - 15.\n",
    "\n",
    "Our assumption is that the centroid of the key played will land in a discrete range with no overlap/ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# ################# Assign ranges\n",
    "# full_key_list = final_labels_bl + final_labels_w\n",
    "# full_key_list = sorted(full_key_list, key=lambda x: x[1].astype(float))\n",
    "# # for ls in full_key_list: \n",
    "# #     ls = ls.reverse()\n",
    "# full_key_list = np.array(full_key_list)\n",
    "# # print(full_key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below maps the actual range for each discrete key.\n",
    "\n",
    "We know that black keys are skinnier than white keys, and we took the median width of the black keys above. For each black key, it's range is ***centroid - black_key_width/2 < x < centroid + black_key_width/2***.\n",
    "\n",
    "For white keys adjacent to black keys, the above axiom provides one of the bounds.\n",
    "\n",
    "For white keys adjacent to white keys, we simply take the mid-way point between their centroids as one of the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_list = np.empty([len(full_key_list), 2], dtype='object')\n",
    "\n",
    "# for i in range(0,len(full_key_list)-1):\n",
    "#     if len(full_key_list[i,0])==1 and len(full_key_list[i+1,0])>1: # White adjacent to black\n",
    "#         test_list[i,1] = full_key_list[i+1,1].astype(float) - key_width_bl/2\n",
    "#     elif len(full_key_list[i,0])>1: # Black key\n",
    "#         test_list[i,1] = full_key_list[i,1].astype(float) + key_width_bl/2\n",
    "#     else: # White key adjacent to white key\n",
    "#         test_list[i,1] = (full_key_list[i,1].astype(float)+full_key_list[i+1,1].astype(float))/2\n",
    "    \n",
    "#     # No change to the actual note (only the distances, above)\n",
    "#     test_list[i,0] = full_key_list[i,0]\n",
    "\n",
    "# #For the last key, just take it to infinity\n",
    "# test_list[-1,1] = np.inf\n",
    "# test_list[-1,0] = full_key_list[-1,0]\n",
    "\n",
    "# full_key_list = test_list\n",
    "# #print(full_key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below **inserts** any given value between our established key ranges. It returns an index where the given value *would* be inserted, which gives us our corresponding key pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, index = key_pressed(full_key_list, 999)\n",
    "# # print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing with Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sample piano image from  youtube\n",
    "# img_notes = cv2.imread(\"images/IMG_very_close_notes.png\")\n",
    "\n",
    "# #converting to gray\n",
    "# gray_notes = cv2.cvtColor(img_notes, cv2.COLOR_BGR2GRAY)\n",
    "# img_notes_rgb = cv2.cvtColor(img_notes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plt.imshow(img_notes_rgb)\n",
    "# # plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*y_cord[1]* is the position of the top of the keyboard. We calculated this using **half** the original image, so we need to add this back in.\n",
    "\n",
    "# I added it back paps =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Cropped\n",
    "# top_keys_index = y_cord[1] + img_notes.shape[0]//2\n",
    "# crop_img_notes = img_notes[20:(int(top_keys_index)-30)] #Crop the top 20 pixels and bottom 30\n",
    "\n",
    "# plt.imshow(crop_img_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "# # # k = 3\n",
    "# # # blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "# # #Using standard threshold to create contrast between white/black keys\n",
    "# # _, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "# _, th_notes = cv2.threshold(crop_img_notes_gray, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(th_notes, cmap = \"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################################\n",
    "# ####### using connected component detection algorithm to separate all the black notes\n",
    "# connectivity = 1\n",
    "# output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "# num_labels = output[0]\n",
    "# labels = output[1]\n",
    "# stats = output[2]\n",
    "# centroids = output[3]\n",
    "\n",
    "# final_labels = []\n",
    "# note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "# output = img_notes_rgb.copy()\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# #For loop only used for displaying \n",
    "# for i in range(1, num_labels):\n",
    "#     x = stats[i, cv2.CC_STAT_LEFT]\n",
    "#     y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "#     w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "#     h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "#     area = stats[i, cv2.CC_STAT_AREA]\n",
    "#     (cX, cY) = centroids[i]\n",
    "#     cY = cY + 20 # We cropped out the first 20 pixels\n",
    "#     if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "#         final_labels.append(i)\n",
    "#         cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "#         dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "#         cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "#         componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "#         note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "#         note_list.append(note)\n",
    "        \n",
    "#         note_played, _ = key_pressed(full_key_list, note.centroid_x)\n",
    "#         cv2.putText(output, note_played, (int(note.centroid_x), int(note.y_dot)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "# #         display_img(\"Output\", output)\n",
    "# #         display_img(\"Connected Component\", componentMask)\n",
    "# #         cv2.waitKey(0)\n",
    "\n",
    "# # print(final_labels)\n",
    "# # cv2.destroyAllWindows()\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(output)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture('videos/Nocturne Opus 9 No 2_trim.mp4')\n",
    "\n",
    "frames = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "seconds_per_frame = fps/frames\n",
    "counter = 0\n",
    "\n",
    "notes_pressed = []\n",
    "\n",
    "keys_timed = []\n",
    "for x in full_key_list:\n",
    "    keys_timed.append([x[0]])\n",
    "\n",
    "\n",
    "keys_timed_update = []\n",
    "for x in full_key_list:\n",
    "    keys_timed_update.append([x[0]])\n",
    "\n",
    "    \n",
    "testing_screen = []\n",
    "testing_mask = []\n",
    "\n",
    "while (camera.isOpened()):\n",
    "    #print(counter)\n",
    "    #print('-'*100)\n",
    "    \n",
    "    #grabbed is a boolean than tells us if there is a valid frame\n",
    "    (grabbed, frame) = camera.read()\n",
    "\n",
    "    frame_number = camera.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    elapsed = frame_number/fps\n",
    "\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    counter += seconds_per_frame\n",
    "        \n",
    "         \n",
    "    frame = imutils.resize(frame,width = keys.shape[1]) #resize or else it won't work\n",
    "    #print(frame.shape)\n",
    "    \n",
    "    crop_frame = frame[20:int(top_keys_index)-50] #Crop the top 20 pixels and bottom 50\n",
    "    \n",
    "    # threshold the cropped and grayed image\n",
    "    crop_frame_gray = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, th_crop_frame = cv2.threshold(crop_frame_gray, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "    #We have a thresholded image for use to use ConnectedComponents on\n",
    "    #####################################################################################\n",
    "    ####### using connected component detection algorithm to separate all the black notes\n",
    "    connectivity = 8\n",
    "    output = cv2.connectedComponentsWithStats(th_crop_frame, connectivity, cv2.CV_32S)\n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "\n",
    "    indices_to_pop = []\n",
    "\n",
    "\n",
    "    output_img = frame.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    \n",
    "#     print(\"-\"*50)\n",
    "#     print(\"new frame\")\n",
    "    i=1\n",
    "    #Loop through all the connected components\n",
    "    while i < len(stats):\n",
    "\n",
    "        curr_connected_w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "\n",
    "        #Determine if the WIDTH is much bigger than the width of a white key and less than 4x? (So we don't get extraneous video text, etc.)\n",
    "        if curr_connected_w > key_width_w*1.25 and curr_connected_w < key_width_w*4:\n",
    "            \n",
    "\n",
    "            #Threshold just the large component of interest\n",
    "            componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "            threshMask = cv2.bitwise_and(crop_frame_gray, crop_frame_gray, mask = componentMask) #Replace this with video frame\n",
    "\n",
    "#             print(\"We are here.\")\n",
    "#             testing_screen.append(frame.copy())\n",
    "#             testing_mask.append(threshMask.copy())\n",
    "            \n",
    "            # Histogram segregation of black/white key\n",
    "            # Grayscale has one channel so we use [0]\n",
    "                #Possible values range from 0 to 256\n",
    "            bin_scaler = 4\n",
    "            hist = cv2.calcHist([threshMask], [0], None, [256/bin_scaler], [1, 256])\n",
    "\n",
    "\n",
    "            #Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image.\n",
    "            T = hist.argmax() * bin_scaler * .9\n",
    "            th1 = threshMask.copy()\n",
    "            th1[th1>T] = 255\n",
    "            th1[th1<T] = 0\n",
    "\n",
    "            #Detect the first set of keys\n",
    "            connectivity = 8\n",
    "            output = cv2.connectedComponentsWithStats(th1, connectivity, cv2.CV_32S)\n",
    "            num_labels_th1 = output[0]\n",
    "            labels_th1 = output[1]\n",
    "            stats_th1 = output[2]\n",
    "            centroids_th1 = output[3]\n",
    "\n",
    "            #Loop through components and determine which ones may be keys\n",
    "            for j in range(1, num_labels_th1):\n",
    "                area = stats_th1[j, cv2.CC_STAT_AREA]\n",
    "                if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                    if j > 1:\n",
    "                        ## We've added another label\n",
    "                        num_labels +=1 \n",
    "                        i +=1\n",
    "\n",
    "                    ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                    #First, cut out the original \"fat\" label\n",
    "                    fat_mask = labels != i\n",
    "                    labels = labels * fat_mask\n",
    "\n",
    "                    #Next, increment each label above the cut one up to accomodate the new label\n",
    "                    higher_mask = labels > i\n",
    "                    labels = labels + higher_mask\n",
    "\n",
    "                    #Then append our segregated key\n",
    "                    new_mask = labels_th1 == j\n",
    "                    new_labels = labels_th1 * new_mask\n",
    "                    new_labels = i * new_labels\n",
    "                    labels = labels + new_labels\n",
    "\n",
    "                    ##Remove the original index for the stats and then add the new one\n",
    "                    if i < len(stats):\n",
    "                        stats = np.delete(stats,i,0)\n",
    "                        stats = np.insert(stats,i,stats_th1[j],0)\n",
    "                    elif j == 1:\n",
    "                        stats = stats[:-1,:]\n",
    "                        stats = np.concatenate((stats,stats_th1[j][None,:]),0)\n",
    "                    else:\n",
    "                        stats = np.concatenate((stats,stats_th1[j][None,:]),0)\n",
    "\n",
    "                    ##Remove the original index for the centroids and then add the new one\n",
    "                    if i < len(centroids):\n",
    "                        centroids = np.delete(centroids,i,0)\n",
    "                        centroids = np.insert(centroids,i,centroids_th1[j],0)\n",
    "                    elif j==1:\n",
    "                        centroids = centroids[:-1,:]\n",
    "                        centroids = np.concatenate((centroids,centroids_th1[j][None,:]),0)  \n",
    "                    else:\n",
    "                        centroids = np.concatenate((centroids,centroids_th1[j][None,:]),0)  \n",
    "                        \n",
    "\n",
    "                        \n",
    "                    print(\"i: {}, stats: {}\".format(i,len(stats)))\n",
    "                    #Plot immediately so indexing doesn't get messed up\n",
    "                    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                    area = stats[i, cv2.CC_STAT_AREA]\n",
    "                    (cX, cY) = centroids[i]\n",
    "                    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                    cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                    dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                    top_dot = cY-dist_to_edge\n",
    "                    bottom_dot = cY+dist_to_edge\n",
    "                    cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                    cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                    note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "            \n",
    "                    if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                        note_played, index = key_pressed(full_key_list, cX)\n",
    "                        #print(str(note_played) + \" at \" + str(counter))\n",
    "                        keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                    if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                        note_played, index = key_pressed(full_key_list, cX)\n",
    "                        keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                    note_played, _ = key_pressed(full_key_list, note.centroid_x)\n",
    "                    cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.y_dot)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #Detect the next set of keys\n",
    "                th2 = threshMask.copy()\n",
    "                th2[th2>T] = 0        \n",
    "                k = 3\n",
    "                blurred_th2 = cv2.GaussianBlur(th2, (k,k), 0)\n",
    "\n",
    "                #Using standard threshold to create contrast between white/black keys\n",
    "                _, th2_notes = cv2.threshold(blurred_th2, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "                #Detect the second set of keys\n",
    "                connectivity = 8\n",
    "                output = cv2.connectedComponentsWithStats(th2_notes, connectivity, cv2.CV_32S)\n",
    "                num_labels_th2 = output[0]\n",
    "                labels_th2 = output[1]\n",
    "                stats_th2 = output[2]\n",
    "                centroids_th2 = output[3]\n",
    "\n",
    "                #Loop through components and determine which ones may be keys\n",
    "                for k in range(1, num_labels_th2):\n",
    "                    area = stats_th2[k, cv2.CC_STAT_AREA]\n",
    "                    if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                        if k > 1:\n",
    "                            ## We've added another label\n",
    "                            num_labels +=1 \n",
    "                            i+=1\n",
    "\n",
    "                        ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                        #For the second key WE DON'T NEED TO CUT anything\n",
    "        #                 fat_mask = labels != i\n",
    "        #                 labels = labels * fat_mask\n",
    "\n",
    "                        #Next, increment each label above the cut one up to accomodate the new label\n",
    "                        higher_mask = labels > i + 1\n",
    "                        labels = labels + higher_mask\n",
    "\n",
    "                        #Then append our segregated key\n",
    "                        new_mask = labels_th2 == k\n",
    "                        new_labels = labels_th2 * new_mask\n",
    "                        new_labels = (i + 1) * new_labels\n",
    "                        labels = labels + new_labels\n",
    "\n",
    "                        ##Add\n",
    "                        if i < len(stats):\n",
    "                            stats = np.insert(stats,(i+1),stats_th2[k],0)                       \n",
    "                        else:\n",
    "                            stats = np.concatenate((stats,stats_th2[k][None,:]),0)\n",
    "                        \n",
    "\n",
    "                        ##Add\n",
    "                        if i < len(centroids):\n",
    "                            centroids = np.insert(centroids,(i+1),centroids_th2[k],0)\n",
    "                        else:\n",
    "                            centroids = np.concatenate((centroids,centroids_th2[k][None,:]),0)\n",
    "\n",
    "                        #Plot immediately so indexing doesn't get messed up\n",
    "                        x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                        y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                        w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                        h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                        area = stats[i, cv2.CC_STAT_AREA]\n",
    "                        (cX, cY) = centroids[i]\n",
    "                        cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                        cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                        dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                        top_dot = cY-dist_to_edge\n",
    "                        bottom_dot = cY+dist_to_edge\n",
    "                        cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                        cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                        note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "            \n",
    "                        if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                            note_played, index = key_pressed(full_key_list, cX)\n",
    "                            #print(str(note_played) + \" at \" + str(counter))\n",
    "                            keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                        if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                            note_played, index = key_pressed(full_key_list, cX)\n",
    "                            keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                        note_played, _ = key_pressed(full_key_list, note.centroid_x)\n",
    "                        cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.y_dot)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "        else:\n",
    "            x = stats[i, cv2.CC_STAT_LEFT]\n",
    "            y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "            w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            (cX, cY) = centroids[i]\n",
    "            cY = cY + 20 # We cropped out the first 20 pixels\n",
    "            if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "                cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                top_dot = cY-dist_to_edge\n",
    "                bottom_dot = cY+dist_to_edge\n",
    "                cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "                if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                    note_played, index = key_pressed(full_key_list, cX)\n",
    "                    #print(str(note_played) + \" at \" + str(counter))\n",
    "                    keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                    note_played, index = key_pressed(full_key_list, cX)\n",
    "                    keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                note_played, _ = key_pressed(full_key_list, note.centroid_x)\n",
    "                cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.y_dot)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    #Show the frame + drawn rectangle\n",
    "    cv2.imshow(\"Video\", output_img)\n",
    "\n",
    "    #Can break early by pressing \"q\"\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# print(keys_timed_update)    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "\n",
    "for i in range (len(keys_timed_update)):\n",
    "    temp = []\n",
    "    temp.append(keys_timed_update[i][0])\n",
    "    if (len(keys_timed_update[i]) > 1):\n",
    "        for j in range(1,len(keys_timed_update[i])):\n",
    "            temp.append(keys_timed_update[i][j][0])\n",
    "\n",
    "    new_list.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(new_list)\n",
    "df.to_csv('notes_info.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "env_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
