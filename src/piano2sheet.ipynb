{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "import mahotas       # Otsu thresholding\n",
    "import bisect        # Key insert point\n",
    "import imutils       # Crop and resize images\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "import os            # Folder paths\n",
    "import sys           # Exit function\n",
    "import glob          # Folder searching\n",
    "\n",
    "from moviepy.editor import VideoFileClip  # Video processing - speeding up\n",
    "import moviepy.video.fx.all as vfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note class\n",
    "class Note: \n",
    "    def __init__(self, centroid_x, y_dot):\n",
    "        self.centroid_x = centroid_x\n",
    "        self.y_dot = y_dot\n",
    "\n",
    "# just  a function for printing images\n",
    "def display_img(title, img):\n",
    "    cv2.imshow(title, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "      \n",
    "# function for getting next note\n",
    "def getNextNote(first_note):\n",
    "    if \"#\" in first_note: \n",
    "        octave = first_note[2]\n",
    "        if first_note[:2] == \"A#\":\n",
    "            return (\"C#\" + octave)\n",
    "        elif first_note[:2] == \"C#\":\n",
    "            return (\"D#\" + octave)\n",
    "        elif first_note[:2] == \"D#\":\n",
    "            return (\"F#\" + octave)\n",
    "        elif first_note[:2] == \"F#\":\n",
    "            return (\"G#\" + octave)\n",
    "        elif first_note[:2] == \"G#\":\n",
    "            next_octave = int(octave) + 1\n",
    "            return (\"A#\" + str(next_octave))\n",
    "    \n",
    "    else: \n",
    "        octave = first_note[1]\n",
    "        if first_note[0] == \"A\":\n",
    "            return (\"B\" + octave)\n",
    "        elif first_note[0] == \"B\":\n",
    "            return (\"C\" + octave)\n",
    "        elif first_note[0] == \"C\":\n",
    "            return (\"D\" + octave)\n",
    "        elif first_note[0] == \"D\":\n",
    "            return (\"E\" + octave)\n",
    "        elif first_note[0] == \"E\":\n",
    "            return (\"F\" + octave)\n",
    "        elif first_note[0] == \"F\":\n",
    "            return (\"G\" + octave)\n",
    "        elif first_note[0] == \"G\":\n",
    "            next_octave = int(octave) + 1\n",
    "            return (\"A\" + str(next_octave))\n",
    "\n",
    "# Function for Gaussian Blurring\n",
    "def gaussianBlurring(gray_img, blur_sq, std_dev = 0):\n",
    "    return cv2.GaussianBlur(gray_img, (blur_sq, blur_sq), std_dev)\n",
    "\n",
    "# Function for Canny Edge Detection\n",
    "def cannyDetection(blurred_img, th1, th2, apertureSize = 3):\n",
    "    return cv2.Canny(blurred_img, th1, th2, apertureSize)\n",
    "\n",
    "# Function to return the top and bottom of the keyboard using HoughLines\n",
    "def keyboardYCoords(edged_img, rho = 1, theta = np.pi/180, threshold = None):\n",
    "    \n",
    "    if threshold is None:\n",
    "        threshold = edged_img.shape[1]//2 # half the width\n",
    "    \n",
    "    lines = cv2.HoughLines(edged_img, rho, theta, threshold) \n",
    "    y_cord = [] #the y-value of the lines generated from hough transform\n",
    "\n",
    "    #iterating through lines\n",
    "    for line in lines: \n",
    "        rho_l, theta_l = line[0]\n",
    "        a = np.cos(theta_l)\n",
    "        b = np.sin(theta_l)\n",
    "        x0 = a * rho_l\n",
    "        y0 = b * rho_l\n",
    "        y_cord.append(y0) #appending to list\n",
    "\n",
    "    y_cord.sort(reverse=True)\n",
    "    return y_cord[0:2]\n",
    "\n",
    "# Function to threshold an image\n",
    "def threshold(gray_img, th1 = 90, th2 = 150, thresh_type = cv2.THRESH_BINARY_INV):\n",
    "    _, threshed_img = cv2.threshold(gray_img, th1, th2, thresh_type)\n",
    "    return threshed_img\n",
    "\n",
    "\n",
    "        \n",
    "# Function for doing connected components\n",
    "def connectedComponents(binarized_img, connectivity = 8, ltype = cv2.CV_32S):\n",
    "    output = cv2.connectedComponentsWithStats(binarized_img, connectivity, cv2.CV_32S)\n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "\n",
    "    return [num_labels, labels, stats, centroids]\n",
    "\n",
    "\n",
    "# Function to calculate the centroids of detected connected components\n",
    "#   This should be used for the 36 black keys and 52 white keys\n",
    "#   use display_result = True to display the results on the source image\n",
    "def keyDetection(orig_img, num_labels, labels, stats, centroids, min_key_area = 100, display_result = False):\n",
    "    final_labels = []\n",
    "\n",
    "    output_img = orig_img.copy()\n",
    "\n",
    "    # Loop through the detected connected components\n",
    "    for i in range(1, num_labels):\n",
    "        [x,y,w,h,area] = getConnectedComponentRectangle(stats[i,:])\n",
    "        (cX, cY) = centroids[i]\n",
    "        \n",
    "        # Consider only connected that have an area greater than min_key_area pixels\n",
    "        if (min_key_area < area < np.inf):\n",
    "            final_labels.append([i,cX])\n",
    "            \n",
    "            # Display on original image\n",
    "            if (display_result):\n",
    "                cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                cv2.circle(output_img, (int(cX), int(cY)), 4, (255,255,0), -1)\n",
    "#                 componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "                cv2.imshow(\"Output\", output_img)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    key_width = statistics.median(stats[:, cv2.CC_STAT_WIDTH])\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    #Return the centroid of the processed connected components and the median width\n",
    "    return final_labels, key_width\n",
    "\n",
    "# Get the x,y coordinates and width + height of cv2.connectedcomponents output\n",
    "#    Given the i-th components of the stats matrix (an N x 5 matrix)\n",
    "def getConnectedComponentRectangle(ith_stats):\n",
    "        x = ith_stats[cv2.CC_STAT_LEFT]\n",
    "        y = ith_stats[cv2.CC_STAT_TOP]\n",
    "        w = ith_stats[cv2.CC_STAT_WIDTH]\n",
    "        h = ith_stats[cv2.CC_STAT_HEIGHT]\n",
    "        area = ith_stats[cv2.CC_STAT_AREA]\n",
    "        \n",
    "        return [x,y,w,h,area]\n",
    "\n",
    "\n",
    "def displayCentroid(key_list, img):\n",
    "    y = img.shape[0]*3//4\n",
    "    for (note, centroid) in key_list: \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        line = cv2.line(img,(int(centroid),0),(int(centroid),900),(0,0,255),1)\n",
    "        text_label = cv2.putText(img, note, (int(centroid), y), font, 0.5, (0,255,0), 1)\n",
    "        cv2.imshow(\"Key Label\", img)\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Given an x-coordinate, return the appropriate insertion point in the keyboard\n",
    "def key_pressed(key_list, key_index):\n",
    "    insertion_point = bisect.bisect_left(key_list[:,1].astype(float),key_index)\n",
    "    \n",
    "    #Insertion outside our index, means to insert it at the end (return the last key)\n",
    "    if insertion_point >= len(key_list):\n",
    "        insertion_point = len(key_list)-1\n",
    "#     print(insertion_point)\n",
    "#     print('You pressed the {} key.'.format(key_list[insertion_point,0]))\n",
    "\n",
    "    note = key_list[insertion_point,0]\n",
    "    index = insertion_point\n",
    "\n",
    "    return note, index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def timeNotes(x,y,w,h, ith_centroids, y_offset, note_height, keyboard_line, keyboard_array, keys_timed_update, \n",
    "              elapsed, output_img, font = cv2.FONT_HERSHEY_SIMPLEX, font_scale = 0.5, font_color = (0,0,255)):\n",
    "    # x,y,w,h - respective x-coord, y-coord, width and height of a connected component\n",
    "    # ith_centroids - the centroid of a given note (i.e. an x,y pair)\n",
    "    # y_offset - if the keyboard has been offset in the y direction (this allows the detection to stay out of the top/bottom edges)\n",
    "    # note_height - from connectedcomponents\n",
    "    # keyboard_line - the y-coordinate that serves as the threshold/line to calculate press down/up\n",
    "    # keyboard_array - the matrix of keyboard notes/octaves and x-coordinates\n",
    "    # key_timed_update - our array that holds on/off button for each note in our keyboard\n",
    "    # elapsed - time in seconds\n",
    "    # output_img - the screen we want to display to\n",
    "\n",
    "    lag = 2 # number of pixels about the keyboard_line for detection\n",
    "    \n",
    "    # Get centroid of detected note\n",
    "    (cX, cY) = ith_centroids\n",
    "    cY = cY + y_offset # We cropped out the first 20 pixels\n",
    "\n",
    "    # Y-coordinate of top and bottom edges of a note (this is truncated by the \"size\" of the detection area)\n",
    "    dist_to_edge = note_height/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "    top_dot = cY-dist_to_edge\n",
    "    bottom_dot = cY+dist_to_edge\n",
    "\n",
    "#     note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "\n",
    "    if ( (int(bottom_dot) >= int(keyboard_line - lag)) and (int(bottom_dot) <= int(keyboard_line)) ):\n",
    "        note_played, index = key_pressed(keyboard_array, cX)\n",
    "        keys_timed_update[index].append([elapsed])\n",
    "#         print(note_played)\n",
    "#         print('='*50)\n",
    "\n",
    "    if ( (int(top_dot) >= int(keyboard_line - lag)) and (int(top_dot) <= int(keyboard_line)) ):\n",
    "        note_played, index = key_pressed(keyboard_array, cX)\n",
    "        keys_timed_update[index].append([elapsed])\n",
    "\n",
    "    note_played, _ = key_pressed(keyboard_array, cX)\n",
    "    cv2.line(output_img, (0, int(keyboard_line)), (600, int(keyboard_line)), (0,0,255), 2)\n",
    "    cv2.rectangle(output_img, (x,y), (x+w, y+h), font_color,1)\n",
    "    cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "    cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "    cv2.putText(output_img, note_played, (int(cX), int(cY+dist_to_edge)), font, font_scale, font_color, 1)\n",
    "    \n",
    "    return keys_timed_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ask user for YouTube link\n",
    "- Download (?) video (or atleast a temporary copy)\n",
    "- Using the first few frames, detect the keyboard\n",
    "    - If 72 keys are not detected, try next few frames and repeat\n",
    "    - If they can't be detected at all, end program\n",
    "- Label black and white keys\n",
    "- Run algorithm, get array with start/end times for each note\n",
    "- Convert to .midi\n",
    "- Convert to sheet music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyboard: \n",
    "    def __init__(self, youtubeURL):\n",
    "        self.youtubeURL = youtubeURL\n",
    "        \n",
    "    # For downloading YouTube videos\n",
    "    def my_hook(self, d):\n",
    "        if d['status'] == 'finished':\n",
    "            print('Download complete.')\n",
    "        elif d['status'] == 'error':\n",
    "            print('Error in downloading file - exiting program!')\n",
    "            sys.exit()\n",
    "            \n",
    "    # Remove previously downloaded file\n",
    "    def clear_previous(self, path = \"./videos/video_to_process*\"):\n",
    "        vtp = glob.glob(path)\n",
    "        if vtp:\n",
    "#             camera = cv2.VideoCapture(vtp[0])\n",
    "#             camera.release()\n",
    "            os.remove(vtp[0])\n",
    "\n",
    "    # Function to download a YouTube video\n",
    "    def downloadYouTube(self, path = './videos/video_to_process.%(ext)s', quiet = True):\n",
    "    \n",
    "        self.clear_previous()\n",
    "    \n",
    "        ydl_opts = {'outtmpl': path,\n",
    "                   'quiet': quiet,\n",
    "                   'progress_hooks': [self.my_hook]}\n",
    "        try:\n",
    "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "                print('Downloading video...')\n",
    "                ydl.download([self.youtubeURL])\n",
    "        except youtube_dl.utils.DownloadError:\n",
    "            print('Exiting program!')\n",
    "            \n",
    "    # Speed up a video by a factor by removing frames\n",
    "    def speed_video(self, spd_factor = 5, path = \"./videos/video_to_process*\"):\n",
    "        vtp = glob.glob(path)\n",
    "        \n",
    "        if vtp:\n",
    "            \n",
    "            # 1) Speed up the video, save as a copy\n",
    "            vtp = vtp[0]\n",
    "            file_ext_idx = vtp.find('.',1)\n",
    "            tmp_vtp = vtp[:file_ext_idx] +\"_COPY\" + vtp[file_ext_idx:]\n",
    "            \n",
    "            in_loc = vtp\n",
    "            out_loc = tmp_vtp\n",
    "\n",
    "            # Import video clip\n",
    "            clip = VideoFileClip(in_loc)\n",
    "#             print(\"fps: {}\".format(clip.fps))\n",
    "\n",
    "            # Modify the FPS\n",
    "#             clip = clip.set_fps(clip.fps * spd_factor)\n",
    "            clip = clip.set_fps(clip.fps * 1)\n",
    "\n",
    "            # Apply speed up\n",
    "            final = clip.fx(vfx.speedx, spd_factor)\n",
    "#             print(\"fps: {}\".format(final.fps))\n",
    "\n",
    "            # Save video clip\n",
    "            final.write_videofile(out_loc)\n",
    "            clip.close()\n",
    "\n",
    "            # 2) Delete the original 1x speed video\n",
    "#             camera = cv2.VideoCapture(vtp)\n",
    "#             camera.release()\n",
    "            os.remove(vtp)\n",
    "            \n",
    "            # 3) Rename the sped-up video to the original\n",
    "            old_file_name = tmp_vtp\n",
    "            new_file_name = vtp\n",
    "            os.rename(old_file_name, new_file_name)\n",
    "            \n",
    "    # Use the file called video_to_process and detect our 88 keys\n",
    "    def detect_keys(self, resize_width, bl_blur_sq, bl_canny_th1, bl_canny_th2, bl_thresh1, bl_thresh2,\n",
    "                wh_blur_sq, path = \"./videos/video_to_process*\",num_bl_keys = 36, num_wh_keys = 52):\n",
    "        \n",
    "        self.resize_width = resize_width\n",
    "        \n",
    "        vtp = glob.glob(path)\n",
    "        if vtp:\n",
    "            camera = cv2.VideoCapture(vtp[0])\n",
    "            # Could not access the file\n",
    "            if not camera.isOpened():\n",
    "                print('Error - video file could not be read!')\n",
    "                sys.exit()\n",
    "            # Process video\n",
    "            else:\n",
    "                print('Detecting keys...')\n",
    "                while True:\n",
    "                    #grabbed is a boolean than tells us if there is a valid frame\n",
    "                    (grabbed, frame) = camera.read()\n",
    "                    if not grabbed:\n",
    "                        break\n",
    "\n",
    "                    frame = imutils.resize(frame,width = resize_width)\n",
    "\n",
    "                    # Get the bottom-half of the frame (where the keyboard lies) and process from here\n",
    "#                     keys = frame[frame.shape[0]//2:,:]\n",
    "#                     gray_keys = cv2.cvtColor(keys, cv2.COLOR_BGR2GRAY)\n",
    "                    keys = frame\n",
    "                    gray_keys = cv2.cvtColor(keys, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # Process\n",
    "                    blurred = gaussianBlurring(gray_keys, blur_sq = bl_blur_sq)\n",
    "                    edges = cannyDetection(blurred, th1 = bl_canny_th1, th2 = bl_canny_th2)\n",
    "\n",
    "                    # Crop\n",
    "                    crop_coordinates  = keyboardYCoords(edges)\n",
    "                    self.y_coords = crop_coordinates\n",
    "                    cropped_keys      = keys[int(crop_coordinates[1])+20:int(crop_coordinates[0])]\n",
    "                    cropped_gray_keys = gray_keys[int(crop_coordinates[1])+20:int(crop_coordinates[0])]\n",
    "\n",
    "                    # Labels keys\n",
    "                    thresh_keys_bl = threshold(cropped_gray_keys, th1 = bl_thresh1, th2 = bl_thresh2)\n",
    "\n",
    "                    # Black keys\n",
    "                    [num_labels_bl, labels_bl, stats_bl, centroids_bl] = connectedComponents(binarized_img = thresh_keys_bl)\n",
    "                    final_labels_bl, key_width_bl = keyDetection(cropped_keys, num_labels_bl, labels_bl, stats_bl, centroids_bl)\n",
    "                    if len(final_labels_bl) == num_bl_keys: \n",
    "                        first_note = \"A#0\"\n",
    "                        for i in range(num_bl_keys):\n",
    "                            final_labels_bl[i][0] = first_note\n",
    "                            first_note = getNextNote(first_note)\n",
    "\n",
    "                        final_labels_bl = sorted(final_labels_bl, key=lambda x: x[1])\n",
    "\n",
    "                    # White keys\n",
    "                    blurred_w = gaussianBlurring(cropped_gray_keys, blur_sq = wh_blur_sq)\n",
    "                    T = mahotas.thresholding.otsu(blurred_w)*1.3\n",
    "                    thresh_keys_w = cropped_gray_keys.copy()\n",
    "                    thresh_keys_w[thresh_keys_w>T] = 255\n",
    "                    thresh_keys_w[thresh_keys_w<T] = 0\n",
    "                    [num_labels_w, labels_w, stats_w, centroids_w] = connectedComponents(binarized_img = thresh_keys_w)\n",
    "                    final_labels_w, key_width_w = keyDetection(cropped_keys, num_labels_w, labels_w, stats_w, centroids_w)\n",
    "\n",
    "                    if len(final_labels_w) == num_wh_keys: \n",
    "                        first_note = \"A0\"\n",
    "                        for j in range(num_wh_keys):\n",
    "                            final_labels_w[j][0] = first_note\n",
    "                            first_note = getNextNote(first_note)\n",
    "\n",
    "                        final_labels_w = sorted(final_labels_w, key=lambda x: x[1])\n",
    "\n",
    "                    # Determine if they sum to 88 keys (36 black, 52 white) - if not, try next frame\n",
    "                    if len(final_labels_bl) == num_bl_keys and len(final_labels_w) == num_wh_keys:\n",
    "                        self.black_keys = final_labels_bl\n",
    "                        self.white_keys = final_labels_w\n",
    "                        self.black_width = key_width_bl\n",
    "                        self.white_width = key_width_w\n",
    "                        self.keyboard_img = frame\n",
    "                        camera.release()         # Release cv2 camera object\n",
    "                        cv2.destroyAllWindows()  # Destroy any cv2 windows\n",
    "                        self.key_ranges()             # Call function to assign end-of-range for each key\n",
    "                        print('Key detection complete.')\n",
    "                        break\n",
    "\n",
    "                #     #Show the frame + drawn rectangle\n",
    "                #     cv2.imshow(\"Face\", thresh_keys_bl)\n",
    "\n",
    "    #                 #Can break early by pressing \"q\"\n",
    "    #                 if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #                     break\n",
    "\n",
    "            # If at this point we've looped through everything and we don't have 72 keys\n",
    "            camera.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            if len(final_labels_bl) != 36 and len(final_labels_w) != 52:\n",
    "                print('Error in processing file - exiting program!')\n",
    "                sys.exit()\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Glob did not find a valid file\n",
    "        else:\n",
    "            print('Error - video path could not be accessed!')\n",
    "            sys.exit()\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Assign end-of-range for each key\n",
    "    def key_ranges(self):\n",
    "\n",
    "        # Array\n",
    "        full_key_list = self.black_keys + self.white_keys\n",
    "        full_key_list = sorted(full_key_list, key=lambda x: x[1].astype(float))\n",
    "        full_key_list = np.array(full_key_list)\n",
    "\n",
    "        # Empty list\n",
    "        tmp_list = np.empty([len(full_key_list), 2], dtype='object')\n",
    "\n",
    "        # Loop through and assign end-of-range for each key\n",
    "        # CASE: white key adjacent to black key: end of the white key range is the adjacent black key's centroid - black/2\n",
    "        #       black key:                       end of the black key range is the black key's centroid + black/2\n",
    "        #       white key adjacent to white key: end of the white key range is the half-way point between the adjacent centroids\n",
    "        for i in range(0,len(full_key_list)-1):\n",
    "            if len(full_key_list[i,0])==1 and len(full_key_list[i+1,0])>1: # White adjacent to black\n",
    "                tmp_list[i,1] = full_key_list[i+1,1].astype(float) - self.black_width/2\n",
    "            elif len(full_key_list[i,0])>1: # Black key\n",
    "                tmp_list[i,1] = full_key_list[i,1].astype(float) + self.black_width/2\n",
    "            else: # White key adjacent to white key\n",
    "                tmp_list[i,1] = (full_key_list[i,1].astype(float)+ full_key_list[i+1,1].astype(float))/2\n",
    "\n",
    "            # No change to the actual note (only the distances, above) for the first key\n",
    "            tmp_list[i,0] = full_key_list[i,0]\n",
    "\n",
    "        #For the last key, just take it to infinity\n",
    "        tmp_list[-1,1] = np.inf\n",
    "        tmp_list[-1,0] = full_key_list[-1,0]\n",
    "\n",
    "        full_key_list = tmp_list\n",
    "\n",
    "        self.keyboard_array = full_key_list\n",
    "        \n",
    "    ################################################################################\n",
    "    def video_process(self, path = \"./videos/video_to_process*\", offset_y_top = 20, offset_y_bot = 40,font = cv2.FONT_HERSHEY_SIMPLEX):\n",
    "     \n",
    "        #offset_y_top - how many pixels we crop from the top of each frame for detection\n",
    "        #offset_y_bot - how many pixels we crop from the bottom of the keyboard_array for detection\n",
    "        #   Both of these ensure our detection only occurs in the middle of each frame - no effect on relative timing of notes\n",
    "        \n",
    "        offset_y_timing = offset_y_bot + 10    \n",
    "        #   Our timing floor is offset from the detection floor by 10 pixels (timing is ABOVE detection)\n",
    "    \n",
    "        vtp = glob.glob(path)\n",
    "        \n",
    "        camera = cv2.VideoCapture(vtp[0])\n",
    "\n",
    "        frames = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "        seconds_per_frame = fps/frames\n",
    "\n",
    "        \n",
    "\n",
    "        keys_timed_update = []\n",
    "        for x in self.keyboard_array:\n",
    "            keys_timed_update.append([x[0]])\n",
    "\n",
    "        while (camera.isOpened()):    \n",
    "            #grabbed is a boolean than tells us if there is a valid frame\n",
    "            (grabbed, frame) = camera.read()\n",
    "\n",
    "            # Calculate the elapsed time (in seconds)\n",
    "            frame_number = camera.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            elapsed = frame_number/fps\n",
    "\n",
    "            if not grabbed:\n",
    "                break       \n",
    "\n",
    "            frame = imutils.resize(frame,width = self.resize_width) #resize or else it won't work\n",
    "\n",
    "            # Our detection range is from: Top of frame + offset_y_top\n",
    "            #                          to: Top of keyboard - offset_y_bot\n",
    "            crop_frame = frame[offset_y_top:int(self.y_coords[1])-offset_y_bot] #Crop the top 20 pixels and bottom 50\n",
    "\n",
    "            # threshold the cropped and grayed image\n",
    "            crop_frame_gray = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2GRAY)\n",
    "            th_crop_frame = threshold(crop_frame_gray, thresh_type = cv2.THRESH_BINARY)\n",
    "\n",
    "            # For each frame, obtain connected components\n",
    "            [num_labels, labels, stats, centroids] = connectedComponents(th_crop_frame)\n",
    "\n",
    "            output_img = frame.copy()\n",
    "            \n",
    "\n",
    "            i=1\n",
    "            #Loop through all the found connected components\n",
    "            while i < len(stats):\n",
    "\n",
    "                # Width of i-th connected component\n",
    "                curr_connected_comp = stats[i, cv2.CC_STAT_WIDTH]\n",
    "\n",
    "                # !!! - Determine if the WIDTH is much bigger than the width of a white key and less than 3x? (So we don't get extraneous video text, etc.)\n",
    "                if curr_connected_comp > self.white_width*1.25 and curr_connected_comp < self.white_width*3:\n",
    "\n",
    "                    #Threshold just the large component of interest\n",
    "                    componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "                    threshMask = cv2.bitwise_and(crop_frame_gray, crop_frame_gray, mask = componentMask) #Replace this with video frame\n",
    "\n",
    "                    # Histogram segregation of black/white key\n",
    "                    # Grayscale has one channel so we use [0]\n",
    "                        #Possible values range from 0 to 256\n",
    "                    bin_scaler = 4\n",
    "                    hist = cv2.calcHist([threshMask], [0], None, [256/bin_scaler], [1, 256])\n",
    "\n",
    "                    #Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image.\n",
    "                    T = hist.argmax() * bin_scaler * .95\n",
    "                    white_notes = threshMask.copy()\n",
    "                    white_notes[white_notes>T] = 255\n",
    "                    white_notes[white_notes<T] = 0\n",
    "\n",
    "                    #Detect the first set of WHITE keys\n",
    "                    [num_labels_wh, labels_wh, stats_wh, centroids_wh] = connectedComponents(white_notes)\n",
    "\n",
    "                    #Loop through components and determine which ones may be keys\n",
    "                    for j in range(1, num_labels_wh):\n",
    "                        area = stats_wh[j, cv2.CC_STAT_AREA]\n",
    "                        # !!! - min white pixel area\n",
    "                        if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                            if j > 1:\n",
    "                                ## We've added another label\n",
    "                                num_labels +=1 \n",
    "                                i +=1\n",
    "\n",
    "                            #Within labels, we have a matrix that is the same size of the image that holds our split component\n",
    "                            #First, cut out the original \"fat\" label - i.e. two or more keys are coupled together\n",
    "                            coupled_keys_mask = labels != i\n",
    "                            labels = labels * coupled_keys_mask\n",
    "\n",
    "                            #Next, increment each label above the cut one up to accomodate the new label\n",
    "                            higher_mask = labels > i\n",
    "                            labels = labels + higher_mask\n",
    "\n",
    "                            #Then append our segregated key\n",
    "                            new_mask = labels_wh == j\n",
    "                            new_labels = labels_wh * new_mask\n",
    "                            new_labels = i * new_labels\n",
    "                            labels = labels + new_labels\n",
    "\n",
    "                            ##Remove the original index for the stats and then add the new one\n",
    "                            if i < len(stats):\n",
    "                                stats = np.delete(stats,i,0)\n",
    "                                stats = np.insert(stats,i,stats_wh[j],0)\n",
    "                            elif j == 1:\n",
    "                                stats = stats[:-1,:]\n",
    "                                stats = np.concatenate((stats,stats_wh[j][None,:]),0)\n",
    "                            else:\n",
    "                                stats = np.concatenate((stats,stats_wh[j][None,:]),0)\n",
    "\n",
    "                            ##Remove the original index for the centroids and then add the new one\n",
    "                            if i < len(centroids):\n",
    "                                centroids = np.delete(centroids,i,0)\n",
    "                                centroids = np.insert(centroids,i,centroids_wh[j],0)\n",
    "                            elif j==1:\n",
    "                                centroids = centroids[:-1,:]\n",
    "                                centroids = np.concatenate((centroids,centroids_wh[j][None,:]),0)  \n",
    "                            else:\n",
    "                                centroids = np.concatenate((centroids,centroids_wh[j][None,:]),0)  \n",
    "\n",
    "\n",
    "                            # Plot immediately so indexing doesn't get messed up\n",
    "                            # !!! - +20 pixels\n",
    "                            [x,y,w,h,area] = getConnectedComponentRectangle(stats[i,:])\n",
    "                            y += offset_y_top # We cropped out the first 20 pixels\n",
    "\n",
    "                            # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "                            if w >= self.black_width * 0.5:\n",
    "                                # For each component, time it and draw via cv2\n",
    "                                keys_timed_update = timeNotes(x,y,w,h,centroids[i], offset_y_top, h, self.y_coords[1]-offset_y_timing, self.keyboard_array, \n",
    "                                                              keys_timed_update, elapsed, output_img,\n",
    "                                                              font = font, font_scale = 0.5, font_color = (128,0,128))\n",
    "\n",
    "                        #Detect the next set of keys\n",
    "                        black_tmp = threshMask.copy()\n",
    "                        black_tmp[black_tmp>T] = 0 # Invert the coupled image used for the white keys to ID black keys        \n",
    "                        blurred_black_notes = gaussianBlurring(black_tmp, blur_sq = 5) # Need to blur to remove extraneous detail\n",
    "                        \n",
    "\n",
    "                        #Using standard threshold segment just the black keys\n",
    "                        black_notes = threshold(blurred_black_notes, thresh_type = cv2.THRESH_BINARY)\n",
    "\n",
    "                        #Detect the second set of keys\n",
    "                        [num_labels_bl, labels_bl, stats_bl, centroids_bl] = connectedComponents(black_notes)\n",
    "\n",
    "\n",
    "                        #Loop through components and determine which ones may be keys\n",
    "                        for k in range(1, num_labels_bl):\n",
    "                            area = stats_bl[k, cv2.CC_STAT_AREA]\n",
    "\n",
    "                            # !!! - min area\n",
    "                            if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                                if k > 1:\n",
    "                                    ## We've added another label\n",
    "                                    num_labels +=1 \n",
    "                                    i+=1\n",
    "\n",
    "                                #For the second set of keys WE DON'T NEED TO CUT anything\n",
    "                #                 coupled_keys_mask = labels != i\n",
    "                #                 labels = labels * coupled_keys_mask\n",
    "\n",
    "                                #Next, increment each label above the cut one up to accomodate the new label\n",
    "                                higher_mask = labels > i + 1\n",
    "                                labels = labels + higher_mask\n",
    "\n",
    "                                #Then append our segregated key\n",
    "                                new_mask = labels_bl == k\n",
    "                                new_labels = labels_bl * new_mask\n",
    "                                new_labels = (i + 1) * new_labels\n",
    "                                labels = labels + new_labels\n",
    "\n",
    "                                ##Add\n",
    "                                if i < len(stats):\n",
    "                                    stats = np.insert(stats,(i+1),stats_bl[k],0)                       \n",
    "                                else:\n",
    "                                    stats = np.concatenate((stats,stats_bl[k][None,:]),0)\n",
    "\n",
    "\n",
    "                                ##Add\n",
    "                                if i < len(centroids):\n",
    "                                    centroids = np.insert(centroids,(i+1),centroids_bl[k],0)\n",
    "                                else:\n",
    "                                    centroids = np.concatenate((centroids,centroids_bl[k][None,:]),0)\n",
    "\n",
    "                                #Plot immediately so indexing doesn't get messed up\n",
    "                                # !!! - +20 pixels\n",
    "                                [x,y,w,h,area] = getConnectedComponentRectangle(stats[i+1,:])\n",
    "                                y += offset_y_top # We cropped out the first 20 pixels\n",
    "\n",
    "\n",
    "                                # !!! - minimum black width\n",
    "                                # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "                                if w >= self.black_width * 0.5:\n",
    "                                    # For each component, time it and draw via cv2\n",
    "                                    keys_timed_update = timeNotes(x,y,w,h,centroids[i+1], offset_y_top, h, self.y_coords[1]-offset_y_timing, self.keyboard_array, \n",
    "                                                              keys_timed_update, elapsed, output_img,\n",
    "                                                              font = font, font_scale = 0.5, font_color = (0,0,255))\n",
    "\n",
    "                else:\n",
    "                    # !!! - +20 pixels\n",
    "                    [x,y,w,h,area] = getConnectedComponentRectangle(stats[i,:])\n",
    "                    y += offset_y_top # We cropped out the first 20 pixels\n",
    "\n",
    "                    # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "                    if (20 < area < np.inf) and w >= self.black_width * 0.5: #filtering out relavent detections (the ones big enough to be keys)\n",
    "                        # For each component, time it and draw via cv2\n",
    "                        keys_timed_update = timeNotes(x,y,w,h,centroids[i], offset_y_top, h, self.y_coords[1]-offset_y_timing, self.keyboard_array, \n",
    "                                                              keys_timed_update, elapsed, output_img,\n",
    "                                                              font = font, font_scale = 0.33, font_color = (255,255,255))\n",
    "\n",
    "                i+=1\n",
    "\n",
    "\n",
    "            #Show the frame + drawn rectangle\n",
    "            cv2.imshow(\"Video\", output_img)\n",
    "\n",
    "            #Can break early by pressing \"q\"\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"p\"):\n",
    "                cv2.waitKey(-1) #wait until any key is pressed\n",
    "\n",
    "        # print(keys_timed_update)    \n",
    "        camera.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.timed_keys = keys_timed_update\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    def export_notes(self):\n",
    "        new_list = []\n",
    "\n",
    "        for i in range (len(self.timed_keys)):\n",
    "            temp = []\n",
    "            temp.append(self.timed_keys[i][0])\n",
    "            if (len(self.timed_keys[i]) > 1):\n",
    "                for j in range(1,len(self.timed_keys[i])):\n",
    "                    temp.append(self.timed_keys[i][j][0])\n",
    "\n",
    "            new_list.append(temp)\n",
    "\n",
    "        df = pd.DataFrame(new_list)\n",
    "        df.to_csv('notes_info.csv', index=False, header=False)\n",
    "    \n",
    "    \n",
    "    # Return the centroid of the black/white keys alongside the median width of each\n",
    "    def getKeys(self):\n",
    "        return [self.black_keys, self.white_keys, self.black_width, self.white_width]\n",
    "    \n",
    "    # Return the frame that 88 keys were successfully identified from\n",
    "    def getFrame(self):\n",
    "        return self.keyboard_img\n",
    "    \n",
    "    def getFullKeyList(self):\n",
    "        return self.keyboard_array\n",
    "    \n",
    "    def getKeyboardYCoords(self):\n",
    "        return self.y_coords\n",
    "    \n",
    "    def getTimedKeys(self):\n",
    "        return self.timed_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video...\n",
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                        | 0/1626 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./videos\\video_to_process_COPY.mp4.\n",
      "MoviePy - Writing audio in video_to_process_COPYTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|                                                                            | 0/2212 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video ./videos\\video_to_process_COPY.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./videos\\video_to_process_COPY.mp4\n",
      "Detecting keys...\n",
      "Key detection complete.\n"
     ]
    }
   ],
   "source": [
    "#Download a YouTube video and process it to determine where the centroid of the black and white keys are\n",
    "keyboard = Keyboard('https://www.youtube.com/watch?v=skFugVOqBM4')\n",
    "keyboard.downloadYouTube()\n",
    "keyboard.speed_video(spd_factor = 5)\n",
    "keyboard.detect_keys(resize_width = 600, bl_blur_sq = 5, bl_canny_th1 = 200, bl_canny_th2 = 200, \n",
    "                    bl_thresh1 = 90, bl_thresh2 = 150, wh_blur_sq = 7)\n",
    "keyboard.video_process()\n",
    "keyboard.export_notes()\n",
    "\n",
    "# [black, white, black_width, white_width] = keyboard.getKeys()     # Array of keys\n",
    "# key_img = keyboard.getFrame()                                     # Image of video used to detect our 88 keys\n",
    "# keyboard_array = keyboard.getFullKeyList()                        # Combined array of keys\n",
    "# keyboard_y_coords = keyboard.getKeyboardYCoords()                 # y[0] is the bottom of the keyboard, y[1] is the top\n",
    "# timed_keys = keyboard.getTimedKeys()                                # Array of timed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to delete the processed video\n",
    "# keyboard.clear_previous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE BELOW THIS LINE ##########################\n",
    "___________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: './videos\\\\video_to_process.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a435bcfb6eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcamera\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcamera\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: './videos\\\\video_to_process.mp4'"
     ]
    }
   ],
   "source": [
    "path = \"./videos/video_to_process*\"\n",
    "vtp = glob.glob(path)\n",
    "\n",
    "vtp[0]\n",
    "camera = cv2.VideoCapture(vtp[0])\n",
    "camera.release()\n",
    "os.remove(vtp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# camera = cv2.VideoCapture('videos/Moonlight Sonata 1st Movement  Opus 27 No 2_Trim.mp4')\n",
    "\n",
    "# frames = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "# fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "# seconds_per_frame = fps/frames\n",
    "\n",
    "\n",
    "# keys_timed_update = []\n",
    "# for x in keyboard_array:\n",
    "#     keys_timed_update.append([x[0]])\n",
    "\n",
    "\n",
    "\n",
    "# while (camera.isOpened()):    \n",
    "#     #grabbed is a boolean than tells us if there is a valid frame\n",
    "#     (grabbed, frame) = camera.read()\n",
    "\n",
    "#     # Calculate the elapsed time (in seconds)\n",
    "#     frame_number = camera.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "#     elapsed = frame_number/fps\n",
    "\n",
    "#     if not grabbed:\n",
    "#         break       \n",
    "         \n",
    "#     #!!! - width\n",
    "#     frame = imutils.resize(frame,width = 600) #resize or else it won't work\n",
    "    \n",
    "#     #!!! - keyboard_y_coords//40//20\n",
    "#     crop_frame = frame[20:int(keyboard_y_coords[1])-40] #Crop the top 20 pixels and bottom 50\n",
    "    \n",
    "#     # threshold the cropped and grayed image\n",
    "#     crop_frame_gray = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2GRAY)\n",
    "#     th_crop_frame = threshold(crop_frame_gray, thresh_type = cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "#     # For each frame, obtain connected components\n",
    "#     [num_labels, labels, stats, centroids] = connectedComponents(th_crop_frame)\n",
    "\n",
    "#     output_img = frame.copy()\n",
    "#     font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    \n",
    "#     i=1\n",
    "#     #Loop through all the found connected components\n",
    "#     while i < len(stats):\n",
    "\n",
    "#         # Width of i-th connected component\n",
    "#         curr_connected_comp = stats[i, cv2.CC_STAT_WIDTH]\n",
    "\n",
    "#         # !!! - Determine if the WIDTH is much bigger than the width of a white key and less than 3x? (So we don't get extraneous video text, etc.)\n",
    "#         if curr_connected_comp > white_width*1.25 and curr_connected_comp < white_width*3:\n",
    "            \n",
    "#             #Threshold just the large component of interest\n",
    "#             componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "#             threshMask = cv2.bitwise_and(crop_frame_gray, crop_frame_gray, mask = componentMask) #Replace this with video frame\n",
    "\n",
    "#             # Histogram segregation of black/white key\n",
    "#             # Grayscale has one channel so we use [0]\n",
    "#                 #Possible values range from 0 to 256\n",
    "#             bin_scaler = 4\n",
    "#             hist = cv2.calcHist([threshMask], [0], None, [256/bin_scaler], [1, 256])\n",
    "\n",
    "#             #Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image.\n",
    "#             T = hist.argmax() * bin_scaler * .95\n",
    "#             white_notes = threshMask.copy()\n",
    "#             white_notes[white_notes>T] = 255\n",
    "#             white_notes[white_notes<T] = 0\n",
    "\n",
    "#             #Detect the first set of WHITE keys\n",
    "#             [num_labels_wh, labels_wh, stats_wh, centroids_wh] = connectedComponents(white_notes)\n",
    "\n",
    "#             #Loop through components and determine which ones may be keys\n",
    "#             for j in range(1, num_labels_wh):\n",
    "#                 area = stats_wh[j, cv2.CC_STAT_AREA]\n",
    "#                 # !!! - min white pixel area\n",
    "#                 if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "#                     if j > 1:\n",
    "#                         ## We've added another label\n",
    "#                         num_labels +=1 \n",
    "#                         i +=1\n",
    "\n",
    "#                     #Within labels, we have a matrix that is the same size of the image that holds our split component\n",
    "#                     #First, cut out the original \"fat\" label - i.e. two or more keys are coupled together\n",
    "#                     coupled_keys_mask = labels != i\n",
    "#                     labels = labels * coupled_keys_mask\n",
    "\n",
    "#                     #Next, increment each label above the cut one up to accomodate the new label\n",
    "#                     higher_mask = labels > i\n",
    "#                     labels = labels + higher_mask\n",
    "\n",
    "#                     #Then append our segregated key\n",
    "#                     new_mask = labels_wh == j\n",
    "#                     new_labels = labels_wh * new_mask\n",
    "#                     new_labels = i * new_labels\n",
    "#                     labels = labels + new_labels\n",
    "\n",
    "#                     ##Remove the original index for the stats and then add the new one\n",
    "#                     if i < len(stats):\n",
    "#                         stats = np.delete(stats,i,0)\n",
    "#                         stats = np.insert(stats,i,stats_wh[j],0)\n",
    "#                     elif j == 1:\n",
    "#                         stats = stats[:-1,:]\n",
    "#                         stats = np.concatenate((stats,stats_wh[j][None,:]),0)\n",
    "#                     else:\n",
    "#                         stats = np.concatenate((stats,stats_wh[j][None,:]),0)\n",
    "\n",
    "#                     ##Remove the original index for the centroids and then add the new one\n",
    "#                     if i < len(centroids):\n",
    "#                         centroids = np.delete(centroids,i,0)\n",
    "#                         centroids = np.insert(centroids,i,centroids_wh[j],0)\n",
    "#                     elif j==1:\n",
    "#                         centroids = centroids[:-1,:]\n",
    "#                         centroids = np.concatenate((centroids,centroids_wh[j][None,:]),0)  \n",
    "#                     else:\n",
    "#                         centroids = np.concatenate((centroids,centroids_wh[j][None,:]),0)  \n",
    "                        \n",
    "                        \n",
    "#                     # Plot immediately so indexing doesn't get messed up\n",
    "#                     # !!! - +20 pixels\n",
    "#                     [x,y,w,h,area] = getConnectedComponentRectangle(stats[i,:])\n",
    "#                     y += 20 # We cropped out the first 20 pixels\n",
    "                    \n",
    "#                     # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "#                     if w >= black_width * 0.5:\n",
    "#                         # For each component, time it and draw via cv2\n",
    "#                         keys_timed_update = timeNotes(x,y,w,h,centroids[i], 20, h, keyboard_y_coords[1]-50, keyboard_array, \n",
    "#                                                       keys_timed_update, elapsed, output_img,\n",
    "#                                                       font = font, font_scale = 0.5, font_color = (128,0,128))\n",
    "\n",
    "#                 #Detect the next set of keys\n",
    "#                 black_tmp = threshMask.copy()\n",
    "#                 black_tmp[black_tmp>T] = 0 # Invert the coupled image used for the white keys to ID black keys        \n",
    "#                 k = 5\n",
    "#                 blurred_black_notes = cv2.GaussianBlur(black_tmp, (k,k), 0) # Need to blur to remove extraneous detail\n",
    "\n",
    "#                 #Using standard threshold segment just the black keys\n",
    "#                 black_notes = threshold(blurred_black_notes, thresh_type = cv2.THRESH_BINARY)\n",
    "\n",
    "#                 #Detect the second set of keys\n",
    "#                 [num_labels_bl, labels_bl, stats_bl, centroids_bl] = connectedComponents(black_notes)\n",
    "                \n",
    "\n",
    "#                 #Loop through components and determine which ones may be keys\n",
    "#                 for k in range(1, num_labels_bl):\n",
    "#                     area = stats_bl[k, cv2.CC_STAT_AREA]\n",
    "                    \n",
    "#                     # !!! - min area\n",
    "#                     if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "#                         if k > 1:\n",
    "#                             ## We've added another label\n",
    "#                             num_labels +=1 \n",
    "#                             i+=1\n",
    "\n",
    "#                         #For the second set of keys WE DON'T NEED TO CUT anything\n",
    "#         #                 coupled_keys_mask = labels != i\n",
    "#         #                 labels = labels * coupled_keys_mask\n",
    "\n",
    "#                         #Next, increment each label above the cut one up to accomodate the new label\n",
    "#                         higher_mask = labels > i + 1\n",
    "#                         labels = labels + higher_mask\n",
    "\n",
    "#                         #Then append our segregated key\n",
    "#                         new_mask = labels_bl == k\n",
    "#                         new_labels = labels_bl * new_mask\n",
    "#                         new_labels = (i + 1) * new_labels\n",
    "#                         labels = labels + new_labels\n",
    "\n",
    "#                         ##Add\n",
    "#                         if i < len(stats):\n",
    "#                             stats = np.insert(stats,(i+1),stats_bl[k],0)                       \n",
    "#                         else:\n",
    "#                             stats = np.concatenate((stats,stats_bl[k][None,:]),0)\n",
    "                        \n",
    "\n",
    "#                         ##Add\n",
    "#                         if i < len(centroids):\n",
    "#                             centroids = np.insert(centroids,(i+1),centroids_bl[k],0)\n",
    "#                         else:\n",
    "#                             centroids = np.concatenate((centroids,centroids_bl[k][None,:]),0)\n",
    "\n",
    "#                         #Plot immediately so indexing doesn't get messed up\n",
    "#                         # !!! - +20 pixels\n",
    "#                         [x,y,w,h,area] = getConnectedComponentRectangle(stats[i+1,:])\n",
    "#                         y += 20 # We cropped out the first 20 pixels\n",
    "\n",
    "                        \n",
    "#                         # !!! - minimum black width\n",
    "#                         # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "#                         if w >= black_width * 0.5:\n",
    "#                             # For each component, time it and draw via cv2\n",
    "#                             keys_timed_update = timeNotes(x,y,w,h,centroids[i+1], 20, h, keyboard_y_coords[1]-50, keyboard_array, \n",
    "#                                                       keys_timed_update, elapsed, output_img,\n",
    "#                                                       font = font, font_scale = 0.5, font_color = (0,0,255))\n",
    "\n",
    "#         else:\n",
    "#             # !!! - +20 pixels\n",
    "#             [x,y,w,h,area] = getConnectedComponentRectangle(stats[i,:])\n",
    "#             y += 20 # We cropped out the first 20 pixels\n",
    "            \n",
    "#             # Ensure our detected note is bigger than the median width of a black key multiplied by a factor\n",
    "#             if (20 < area < np.inf) and w >= black_width * 0.5: #filtering out relavent detections (the ones big enough to be keys)\n",
    "#                 # For each component, time it and draw via cv2\n",
    "#                 keys_timed_update = timeNotes(x,y,w,h,centroids[i], 20, h, keyboard_y_coords[1]-50, keyboard_array, \n",
    "#                                                       keys_timed_update, elapsed, output_img,\n",
    "#                                                       font = font, font_scale = 0.33, font_color = (255,255,255))\n",
    "\n",
    "#         i+=1\n",
    "    \n",
    "    \n",
    "#     #Show the frame + drawn rectangle\n",
    "#     cv2.imshow(\"Video\", output_img)\n",
    "\n",
    "#     #Can break early by pressing \"q\"\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "        \n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"p\"):\n",
    "#         cv2.waitKey(-1) #wait until any key is pressed\n",
    "\n",
    "# # print(keys_timed_update)    \n",
    "# camera.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "env_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
