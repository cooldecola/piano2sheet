{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics and image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics\n",
    "import mahotas\n",
    "import bisect\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video processing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "import os            # Folder paths\n",
    "import sys           # Exit function\n",
    "import glob          # Folder searching\n",
    "\n",
    "from moviepy.editor import VideoFileClip  # Video processing - speeding up\n",
    "import moviepy.video.fx.all as vfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video/image processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImage(title, image):\n",
    "    \"\"\"\n",
    "\n",
    "    Use opencv's functionality to display an image\n",
    "\n",
    "    Args:\n",
    "        title (str): A title for the pop-up window\n",
    "        image      : Image to be displayed\n",
    "\n",
    "    \"\"\"\n",
    "    cv2.imshow(title, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    \n",
    "def displayCentroid(key_list, img):\n",
    "    \"\"\"\n",
    "\n",
    "    Use opencv's functionality to display detected keys on the keyboard\n",
    "\n",
    "    Args:\n",
    "        key_list (str, float) : Array of notes and their x-coordinate\n",
    "        img (image)           : Image to be displayed\n",
    "\n",
    "    \"\"\"\n",
    "    img_height = img.shape[0]\n",
    "    display_heights = [img_height * 1 // 5, img_height * 2 // 5, img_height * 3 // 5, img_height * 4 // 5]\n",
    "    counter = 0;\n",
    "        \n",
    "    # Loop through all keys\n",
    "    for (note, centroid) in key_list: \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        line = cv2.line(img,(int(centroid),0),(int(centroid),img_height),(0,0,255),1)\n",
    "        text_label = cv2.putText(img, note, (int(centroid), display_heights[counter % 4]), font, 0.5, (0,255,0), 1)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        cv2.imshow(\"Key Label\", img)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect keys, given a binarized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConnectedComponents(binarized_img, connectivity = 4):\n",
    "    \"\"\"\n",
    "\n",
    "    Given a binarized (black and white) image, detect components with a given connectivity value\n",
    "\n",
    "    Args:\n",
    "        binarized_img (image): the 8-bit single-channel image to be labeled\n",
    "        connectivity (int)   : 8 or 4 for 8-way or 4-way connectivity respectively\n",
    "\n",
    "    Returns:\n",
    "        num_labels (int)           : Number of detected components\n",
    "        labels (int)               : \n",
    "        stats (array of floats)    : Array with the components x, y, width, height and area\n",
    "        centroids (array of tuples): Array with the components' centroid (x,y coordinates)\n",
    "    \"\"\"\n",
    "    connections = cv2.connectedComponentsWithStats(binarized_img, connectivity, cv2.CV_32S)\n",
    "    \n",
    "    num_labels = connections[0]\n",
    "    labels = connections[1]\n",
    "    stats = connections[2]\n",
    "    centroids = connections[3]\n",
    "    \n",
    "    return num_labels, labels, stats, centroids\n",
    "\n",
    "def getKeyStats(index, stats_array):\n",
    "    \"\"\"\n",
    "\n",
    "    Given stats from cv2's connectedComponents' function\n",
    "\n",
    "    Args:\n",
    "        index (int)                   : Index of the array\n",
    "        stats_array (array of floats) : Array with the components x, y, width, height and area\n",
    "\n",
    "    Returns:\n",
    "        x (float)    : Coordinates (from the top-left) of the component\n",
    "        y (float)    : \n",
    "        w (float)    : Width (in pixels)\n",
    "        h (float)    : Height (in pixels)\n",
    "        area (float) : Area (in pixels)\n",
    "    \"\"\"\n",
    "    x = stats_array[index, cv2.CC_STAT_LEFT]\n",
    "    y = stats_array[index, cv2.CC_STAT_TOP]\n",
    "    w = stats_array[index, cv2.CC_STAT_WIDTH]\n",
    "    h = stats_array[index, cv2.CC_STAT_HEIGHT]\n",
    "    area = stats_array[index, cv2.CC_STAT_AREA]\n",
    "    \n",
    "    return x, y, w, h, area\n",
    "    \n",
    "def displayDetectedKeys(image, labels, index, x, y, width, height, centroid_x, centroid_y):\n",
    "    \"\"\"\n",
    "\n",
    "    Display the detected keys (and its associated centroid) on the original image\n",
    "\n",
    "    \"\"\"\n",
    "    cv2.rectangle(image, (x,y), (x+width, y+height), (0,0,255),1)\n",
    "    cv2.circle(image, (int(centroid_x), int(centroid_y)), 4, (255,255,0), -1)\n",
    "    componentMask = (labels == index).astype(\"uint8\") * 255\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    \n",
    "# function for doing connected components\n",
    "def findKeys(binarized_img, original_img, display_result, min_key_area = 100):\n",
    "    \"\"\"\n",
    "\n",
    "    Detect the keys in a given image\n",
    "\n",
    "    Args:\n",
    "        binarized_img (image) : Binarized image\n",
    "        original_img (image)  : Original image (for displaying back onto)\n",
    "        display_result (bool) : True if displaying results is desired\n",
    "        min_key_area (int)    : Minimum area to be considered a key\n",
    "\n",
    "    Returns:\n",
    "        key_array (int, float)   : Index of the key and its' associated x-coordinate centroid\n",
    "        key_median_width (float) : Median width of all detected keys\n",
    "        \n",
    "    \"\"\"\n",
    "    key_array = []\n",
    "    \n",
    "    num_labels, labels, stats, centroids = getConnectedComponents(binarized_img)\n",
    "    key_median_width = statistics.median(stats[:, cv2.CC_STAT_WIDTH])\n",
    "   \n",
    "    # Loop through all the identified components\n",
    "    for i in range(1, num_labels):\n",
    "        # Get statistics for each detected component\n",
    "        # key_x...h are the rectangular values associated (from the top-left)\n",
    "        # cX, cY is the centroid of the detected component\n",
    "        key_x, key_y, key_w, key_h, key_area = getKeyStats(i, stats)\n",
    "        (cX, cY) = centroids[i]\n",
    "        \n",
    "        # If the area of a component is over a threshold, assume it is a key\n",
    "        if (min_key_area < key_area < np.inf):\n",
    "            key_array.append([i,cX])\n",
    "            \n",
    "            # If the user wants to use cv2 to display the detected keys on the image\n",
    "            if (display_result):\n",
    "                displayDetectedKeys(original_img.copy(), labels, i, key_x, key_y, key_w, key_h, cX, cY)\n",
    "                \n",
    "    cv2.destroyAllWindows()\n",
    "    return key_array, key_median_width\n",
    "\n",
    "\n",
    "def getBlurredimage(image, kernel_sz = 3, std_dev = 0):\n",
    "    \"\"\"\n",
    "\n",
    "    Use cv2.GaussianBlur on an image\n",
    "        \n",
    "    \"\"\"\n",
    "    return cv2.GaussianBlur(image, (kernel_sz,kernel_sz), std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextNote(note):\n",
    "    \"\"\"\n",
    "\n",
    "    Given a note, get the next note in the same key.\n",
    "    E.g. B#3 return C#3\n",
    "\n",
    "    Args:\n",
    "        note (str): A note with a key and an octave\n",
    "\n",
    "    Returns:\n",
    "        str: The next note in the same key\n",
    "    \"\"\"\n",
    "    sharp = \"#\"\n",
    "    flat = \"â™­\"\n",
    "    \n",
    "    _note = note[:1]\n",
    "    octave = note[-1]\n",
    "    \n",
    "    if sharp in note:\n",
    "        key = sharp\n",
    "        match _note:\n",
    "            case \"A\":\n",
    "                return \"C\" + key + octave\n",
    "            case \"C\":\n",
    "                return \"D\" + key + octave\n",
    "            case \"D\":\n",
    "                return \"F\" + key + octave\n",
    "            case \"F\":\n",
    "                return \"G\" + key + octave\n",
    "            case \"G\":\n",
    "                return \"A\" + key + str(int(octave) + 1)\n",
    "            \n",
    "    elif flat in note:\n",
    "        key = flat\n",
    "        match _note:\n",
    "            case \"A\":\n",
    "                return \"B\" + key + octave\n",
    "            case \"B\":\n",
    "                return \"D\" + key + octave\n",
    "            case \"D\":\n",
    "                return \"E\" + key + octave\n",
    "            case \"E\":\n",
    "                return \"G\" + key + octave\n",
    "            case \"G\":\n",
    "                return \"A\" + key + str(int(octave) + 1)\n",
    "    else: # white keys\n",
    "        key = \"\";\n",
    "        match _note:\n",
    "            case \"A\":\n",
    "                return \"B\" + key + octave\n",
    "            case \"B\":\n",
    "                return \"C\" + key + octave\n",
    "            case \"C\":\n",
    "                return \"D\" + key + octave\n",
    "            case \"D\":\n",
    "                return \"E\" + key + octave\n",
    "            case \"E\":\n",
    "                return \"F\" + key + octave\n",
    "            case \"F\":\n",
    "                return \"G\" + key + octave\n",
    "            case \"G\":\n",
    "                return \"A\" + key + str(int(octave) + 1)\n",
    "\n",
    "            \n",
    "        \n",
    "def getKeyPressed(key_list, pressed_key_coordinate):\n",
    "    \"\"\"\n",
    "\n",
    "    Determine which key has been pressed\n",
    "\n",
    "    Args:\n",
    "        key_list (str, float)          : Array of notes and their x-coordinate\n",
    "        pressed_key_coordinate (float) : X-coordinate of the most recently pressed key\n",
    "\n",
    "    Returns:\n",
    "        note (str)  : The note that has been pressed\n",
    "        index (int) : The index in the array associated with said note\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the location in the array where the key has been pressed\n",
    "    insertion_point = bisect.bisect_left(key_list[:,1].astype(float), pressed_key_coordinate)\n",
    "    \n",
    "    #Insertion is outside of our array - insert it at the end (i.e. return the last key)\n",
    "    if insertion_point >= len(key_list):\n",
    "        insertion_point = len(key_list)-1\n",
    "\n",
    "    note = key_list[insertion_point,0]\n",
    "    index = insertion_point\n",
    "\n",
    "    return note, index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "class Note: \n",
    "    # Provide an x and y coordinate for the note\n",
    "    def __init__(self, x, y):\n",
    "        self.centroid_x = x\n",
    "        self.centroid_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncorrectNumberOfKeysException(Exception):\n",
    "    pass\n",
    "\n",
    "class Keyboard:\n",
    "    def __init__(self, showAnimations = False):\n",
    "        self.showAnimations = showAnimations\n",
    "    \n",
    "    def detectKeys(self, image = None):\n",
    "        self.readKeyboard(image)\n",
    "        self._detectKeys()\n",
    "                \n",
    "    def readKeyboard(self, image):\n",
    "        if image is None:\n",
    "            # Read image of a static keyboard image\n",
    "            self.keyboard_images = cv2.imread(root + \"images/synthesia.png\")\n",
    "        else:\n",
    "            self.keyboard_images = image\n",
    "\n",
    "        self.thresholdKeyboard()\n",
    "    \n",
    "    def thresholdKeyboard(self):\n",
    "        # Convert to grayscale\n",
    "        gray_keys = cv2.cvtColor(self.keyboard_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Edge detection on a blurred image\n",
    "        blurred_keys = getBlurredimage(gray_keys)\n",
    "        edges = cv2.Canny(blurred_keys, 200,200, apertureSize = 3)\n",
    "\n",
    "        # Find which edges are lines\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, 300) \n",
    "        \n",
    "        self.getPianoCoordinateRange(lines)\n",
    "\n",
    "        # Crop image\n",
    "        self.cropped_keys = self.keyboard_images[self.top_of_piano:self.bottom_of_piano]\n",
    "        self.cropped_gray_keys = gray_keys[self.top_of_piano:self.bottom_of_piano]\n",
    "    \n",
    "    \n",
    "        # Threshold image\n",
    "        _, self.thresholded_keyboard = cv2.threshold(self.cropped_gray_keys, 90, 150, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "    def getPianoCoordinateRange(self, lines):\n",
    "        # Helper function that determines the y-coordinates of the top and the bottom of the keys\n",
    "        y_cord = [] #the y-value of the lines generated from hough transform\n",
    "\n",
    "        #iterating through lines\n",
    "        for line in lines:\n",
    "            rho, theta = line[0]\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            y_cord.append(y0) #appending to list\n",
    "\n",
    "\n",
    "        self.top_of_piano = int(min(y_cord))\n",
    "        self.bottom_of_piano = int(max(y_cord))\n",
    "        \n",
    "    \n",
    "    def _detectKeys(self):\n",
    "        self.detectBlackKeys()\n",
    "        self.detectWhiteKeys()\n",
    "        self.assignKeyRanges()\n",
    "    \n",
    "    def detectBlackKeys(self):\n",
    "        # Use 'True' to see an animation of the detection\n",
    "        black_keys, self.black_key_width = findKeys(self.thresholded_keyboard, self.cropped_keys, self.showAnimations)\n",
    "\n",
    "        # Label the black keys\n",
    "        if len(black_keys) == 36: \n",
    "            note = \"A#0\"\n",
    "            for i in range(36):\n",
    "                black_keys[i][0] = note\n",
    "                note = getNextNote(note)\n",
    "        else:\n",
    "            raise IncorrectNumberOfKeysException('Only ' + str(len(black_keys)) + 'black keys were detected!')\n",
    "                \n",
    "        self.black_keys = sorted(black_keys, key=lambda x: x[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Uncomment for a different animation\n",
    "        # displayCentroid(black_keys, self.keyboard_images)\n",
    "    \n",
    "    def detectWhiteKeys(self):\n",
    "        # Invert the image used to detect the black keys, with some minor modifications\n",
    "        blurred_keys = getBlurredimage(self.cropped_gray_keys, kernel_sz = 7)\n",
    "        otsu_threshold = mahotas.thresholding.otsu(blurred_keys)*1.3\n",
    "        white_thresholded_keyboard = self.cropped_gray_keys.copy()\n",
    "        white_thresholded_keyboard[white_thresholded_keyboard>otsu_threshold] = 255\n",
    "        white_thresholded_keyboard[white_thresholded_keyboard<otsu_threshold] = 0\n",
    "\n",
    "        white_keys, self.white_key_width = findKeys(white_thresholded_keyboard, self.cropped_keys, self.showAnimations)\n",
    "\n",
    "        # Label the white keys\n",
    "        if len(white_keys) == 52: \n",
    "            note = \"A0\"\n",
    "            for i in range(52):\n",
    "                white_keys[i][0] = note\n",
    "                note = getNextNote(note)\n",
    "        else:\n",
    "            raise IncorrectNumberOfKeysException('Only ' + str(len(white_keys)) + 'white keys were detected!')\n",
    "                \n",
    "        self.white_keys = sorted(white_keys, key=lambda x: x[1])\n",
    "\n",
    "        ## Uncomment for a different animation\n",
    "        # displayCentroid(white_keys, self.keyboard_images)\n",
    "    \n",
    "    def assignKeyRanges(self):\n",
    "        # Concatenate the black and white keys\n",
    "        keys = self.black_keys + self.white_keys\n",
    "        keys = sorted(keys, key=lambda x: x[1].astype(float))\n",
    "        keys = np.array(keys)\n",
    "        \n",
    "        keys_ = np.empty([len(keys), 2], dtype='object')\n",
    "        \n",
    "        for i in range(0,len(keys)-1):\n",
    "            \n",
    "            currentKey = keys[i,0]   # The string associated with the key\n",
    "            nextKey    = keys[i+1,0]\n",
    "            \n",
    "            currentKeyPosition  = keys[i, 1]\n",
    "            nextKeyPosition    = keys[i+1,1]\n",
    "                        \n",
    "            if (self.whiteNextToBlack(currentKey, nextKey)):   # White adjacent to a black key\n",
    "                keys_[i,1] = nextKeyPosition.astype(float) - self.black_key_width/2\n",
    "            elif (self.blackNextToBlack(currentKey, nextKey)): # Black key adjacent to a black key\n",
    "                keys_[i,1] = currentKeyPosition.astype(float) + self.black_key_width/2\n",
    "            else:                                              # White key adjacent to white key\n",
    "                keys_[i,1] = (currentKeyPosition.astype(float) + nextKeyPosition.astype(float))/2\n",
    "\n",
    "            # No change to the actual note (only the distances, above)\n",
    "            keys_[i,0] = keys[i,0]\n",
    "\n",
    "        #For the last key, just take it to infinity\n",
    "        keys_[-1,1] = np.inf\n",
    "        keys_[-1,0] = keys[-1,0]\n",
    "\n",
    "        keys = keys_\n",
    "\n",
    "        self.keys = keys\n",
    "        \n",
    "    def whiteNextToBlack(self, currentKey, nextKey):\n",
    "        # A white key has the form [A-F][0-8], for example A7\n",
    "        # A black key has the form [A-F][#/â™­][0-8]\n",
    "        return len(currentKey)==2 and len(nextKey)>2\n",
    "        \n",
    "    def blackNextToBlack(self, currentKey, nextKey):\n",
    "        return len(currentKey)>2 and len(nextKey)>2\n",
    "\n",
    "    def getKeyboardImage(self):\n",
    "        return self.keyboard_images\n",
    "    \n",
    "    def getThresholdedKeyboard(self):\n",
    "        return self.thresholded_keyboard\n",
    "    \n",
    "    def getKeys(self):\n",
    "        return self.keys\n",
    "    \n",
    "    def getKeyboardRange(self):\n",
    "        return self.top_of_piano, self.bottom_of_piano\n",
    "    \n",
    "    def isKeyboardDetected(self):\n",
    "        return len(self.black_keys) == 36 and len(self.white_keys) == 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoReadException(Exception):\n",
    "    pass\n",
    "\n",
    "class VideoDownloader: \n",
    "    def __init__(self, youtubeURL):\n",
    "        self.youtubeURL = youtubeURL\n",
    "        \n",
    "    # For downloading YouTube videos\n",
    "    def my_hook(self, d):\n",
    "        if d['status'] == 'finished':\n",
    "            print('Download complete.')\n",
    "        elif d['status'] == 'error':\n",
    "            print('Error in downloading file - exiting program!')\n",
    "            sys.exit()\n",
    "            \n",
    "    # Remove previously downloaded file\n",
    "    def clear_previous(self, path = \"./videos/video_to_process*\"):\n",
    "        video = glob.glob(path)\n",
    "        if video:\n",
    "            os.remove(video[0])\n",
    "\n",
    "    # Function to download a YouTube video\n",
    "    def downloadYouTube(self, path = './videos/video_to_process.%(ext)s', quiet = True):\n",
    "            \n",
    "#         self.clear_previous()\n",
    "        self.clear_previous(path[:-8] + \"*\")\n",
    "    \n",
    "        ydl_opts = {'outtmpl': path,\n",
    "                   'quiet': quiet,\n",
    "                   'noplaylist' : True,\n",
    "                   'format': 'mp4[height=360]',\n",
    "#                     'format': 'bestvideo[height<=360]/worstvideo',\n",
    "                   'progress_hooks': [self.my_hook]}\n",
    "        try:\n",
    "            with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "                ydl.cache.remove()\n",
    "                print('Downloading video...')\n",
    "                ydl.download([self.youtubeURL])\n",
    "        except youtube_dl.utils.DownloadError:\n",
    "            print('Exiting program!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "currwd = os.getcwd()\n",
    "root = currwd[0:currwd.find('testing')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download a video\n",
    "\n",
    "First, we download a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=gHKcUAMU2Lg\"\n",
    "videoDownloader = VideoDownloader(url)\n",
    "# videoDownloader.downloadYouTube(path = root + \"videos\\\\video_to_process.%(ext)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load image in\n",
    "\n",
    "Using this video, cycle through the frames and pass that into the Keyboard class until we detect 36 black keys and 52 white keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keyboard = Keyboard(showAnimations = False)\n",
    "\n",
    "path = root + \"videos\\\\video_to_process*\"\n",
    "vtp = glob.glob(path)\n",
    "if vtp:\n",
    "    camera = cv2.VideoCapture(vtp[0])\n",
    "    frames = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    \n",
    "    isKeyboardDetected = False\n",
    "    frameCounter = 0\n",
    "    \n",
    "    while (not isKeyboardDetected) or (frameCounter < frames):\n",
    "        camera.set(cv2.CAP_PROP_POS_FRAMES, frameCounter)\n",
    "        res, frame = camera.read()\n",
    "        try:        \n",
    "            keyboard.detectKeys(frame)\n",
    "            isKeyboardDetected = keyboard.isKeyboardDetected()\n",
    "            break\n",
    "        except IncorrectNumberOfKeysException as error:\n",
    "            frameCounter += round(frames / 500.0) * 500.0\n",
    "            \n",
    "    if (not isKeyboardDetected) or (frameCounter > frames):\n",
    "        raise IncorrectNumberOfKeysException('The keys could not be read in this video!')\n",
    "    else:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(keyboard.getThresholdedKeyboard())\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    raise VideoReadException('Video could not be read!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be deleted..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the range x:0 -> end, we assign a specify range to each key. For example, A: 0 - 10 pixels, A#: 10 - 15 pixels...\n",
    "\n",
    "Our assumption is that the centroid of the key played will land in a discrete range with no overlap/ambiguity.\n",
    "\n",
    "We know that black keys are skinnier than white keys, and we took the median width of the black keys above. For each black key, it's range is ***centroid - black_key_width/2 < x < centroid + black_key_width/2***.\n",
    "\n",
    "For white keys adjacent to black keys, the above axiom provides one of the bounds.\n",
    "\n",
    "For white keys adjacent to white keys, we simply take the mid-way point between their centroids as one of the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A0' 9.990322580645161]\n",
      " ['A#0' 17.103216300145764]\n",
      " ['B0' 24.965888665529178]\n",
      " ['C0' 32.0096463022508]\n",
      " ['C#0' 39.64021966950064]\n",
      " ['D0' 46.957928802588995]\n",
      " ['D#0' 53.76117299025155]\n",
      " ['E0' 61.63172315206385]\n",
      " ['F0' 68.8628762541806]\n",
      " ['F#0' 76.31559654293189]\n",
      " ['G0' 82.016077170418]\n",
      " ['G#0' 89.2548470958473]\n",
      " ['A1' 95.9935691318328]\n",
      " ['A#1' 102.79410966879706]\n",
      " ['B1' 110.65515875858665]\n",
      " ['C1' 117.99679487179488]\n",
      " ['C#1' 125.52652810740378]\n",
      " ['D1' 132.5818181818182]\n",
      " ['D#1' 139.5833426287414]\n",
      " ['E1' 147.6419481009391]\n",
      " ['F1' 154.5839416058394]\n",
      " ['F#1' 162.1669708029197]\n",
      " ['G1' 168.01277955271564]\n",
      " ['G#1' 175.2531982869961]\n",
      " ['A2' 181.99032258064517]\n",
      " ['A#2' 188.66084176369537]\n",
      " ['B2' 196.5235141290788]\n",
      " ['C2' 203.98402555910542]\n",
      " ['C#2' 211.51280843958887]\n",
      " ['D2' 218.5441176470588]\n",
      " ['D#2' 225.42274375503627]\n",
      " ['E2' 233.50019949461364]\n",
      " ['F2' 240.57142857142858]\n",
      " ['F#2' 248.1552795031056]\n",
      " ['G2' 254.0128205128205]\n",
      " ['G#2' 261.24789961811234]\n",
      " ['A3' 267.96774193548384]\n",
      " ['A#3' 274.78016726403825]\n",
      " ['B3' 282.6480404823428]\n",
      " ['C3' 289.9741935483871]\n",
      " ['C#3' 297.5078924342297]\n",
      " ['D3' 304.5313653136531]\n",
      " ['D#3' 311.4163675883334]\n",
      " ['E3' 319.3646808747117]\n",
      " ['F3' 326.54612546125463]\n",
      " ['F#3' 334.1381420957067]\n",
      " ['G3' 340.0096463022508]\n",
      " ['G#3' 347.24049194093436]\n",
      " ['A4' 353.59420289855075]\n",
      " ['A#4' 360.59115882632454]\n",
      " ['B4' 368.64967753208794]\n",
      " ['C4' 375.60869565217394]\n",
      " ['C#4' 383.3152173913044]\n",
      " ['D4' 390.5]\n",
      " ['D#4' 397.3994140625]\n",
      " ['E4' 405.287109375]\n",
      " ['F4' 412.52044609665427]\n",
      " ['F#4' 420.12530241340653]\n",
      " ['G4' 426.0063897763578]\n",
      " ['G#4' 433.2420483913636]\n",
      " ['A5' 439.56934306569343]\n",
      " ['A#5' 446.43535646435356]\n",
      " ['B5' 454.50019949461364]\n",
      " ['C5' 461.5781818181818]\n",
      " ['C#5' 469.3017262881523]\n",
      " ['D5' 476.45387453874537]\n",
      " ['D#5' 483.3763513318727]\n",
      " ['E5' 491.36341000570485]\n",
      " ['F5' 498.4686346863469]\n",
      " ['F#5' 506.0119769176415]\n",
      " ['G5' 512.0]\n",
      " ['G#5' 519.2388535031847]\n",
      " ['A6' 525.5567765567765]\n",
      " ['A#6' 532.4303490627019]\n",
      " ['B6' 540.367547828848]\n",
      " ['C6' 547.5714285714286]\n",
      " ['C#6' 555.2983496647756]\n",
      " ['D6' 562.4686346863468]\n",
      " ['D#6' 569.3837314056734]\n",
      " ['E6' 577.3618124364837]\n",
      " ['F6' 584.4686346863468]\n",
      " ['F#6' 592.0093706480775]\n",
      " ['G6' 597.9967741935484]\n",
      " ['G#6' 605.2272006560962]\n",
      " ['A7' 611.5]\n",
      " ['A#7' 618.3994140625]\n",
      " ['B7' 626.8994140625]\n",
      " ['C7' inf]]\n"
     ]
    }
   ],
   "source": [
    "print(keyboard.getKeys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below **inserts** any given value between our established key ranges. It returns an index where the given value *would* be inserted, which gives us our corresponding key pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note, index = getKeyPressed(full_key_list, 999)\n",
    "# print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Scenario Testing - Notes are FULLY vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample piano image from youtube\n",
    "img_notes = cv2.imread(root + \"images\\IMG_fully_vertical.png\")\n",
    "\n",
    "#converting to gray\n",
    "gray_notes = cv2.cvtColor(img_notes, cv2.COLOR_BGR2GRAY)\n",
    "img_notes_rgb = cv2.cvtColor(img_notes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_notes_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*y_cord[1]* is the position of the top of the keyboard. We calculated this using **half** the original image, so we need to add this back in.\n",
    "\n",
    "# I added it back paps =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropped\n",
    "top_keys_index = y_cord[1] + img_notes.shape[0]//2\n",
    "crop_img_notes = img_notes[20:(int(top_keys_index)-30)] #Crop the top 20 pixels and bottom 30\n",
    "crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(crop_img_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "# # k = 3\n",
    "# # blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "# #Using standard threshold to create contrast between white/black keys\n",
    "# _, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "# _, th_notes = cv2.threshold(crop_img_notes_gray, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(crop_img_notes_gray, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k = 7 #This level of blurring made it possible to just view the notes\n",
    "# blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "# t1 = 15\n",
    "# t2 = 105\n",
    "# edged = cv2.Canny(blurred, t1, t2)\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(edged, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1a - Iterating on Contours\n",
    "If we have a rectangle that spans the whole screen - the caps are not detected. We need to create our **own** caps. This method uses Canny-edge detection up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Count notes\n",
    "# (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = sorted(cnts, key = cv2.contourArea, reverse = True)\n",
    "\n",
    "# #Each list within cnts is the list of contour points - 1 for each coin (total = 9)\n",
    "# print(\"# of notes: {}\".format(len(cnts)))\n",
    "\n",
    "# #Do it on a copy\n",
    "# tmp = crop_img_notes.copy()\n",
    "\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "\n",
    "# problem = []\n",
    "# i = 0\n",
    "# for cnt in cnts:\n",
    "    \n",
    "#     x,y,w,h = cv2.boundingRect(cnt)\n",
    "#     cv2.rectangle(tmp,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "#     cv2.putText(tmp, str(w), (int(x), int(y+12)), font, 0.25, (255,0,0), 1)\n",
    "    \n",
    "#     if w < key_width_bl*0.9:\n",
    "#         problem.append(cnt)\n",
    "    \n",
    "#     i+=1\n",
    "    \n",
    "# plt.figure(figsize = (10,10))\n",
    "# plt.imshow(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the contours with a width < 90%(?) of our calculated black key width. We extract and figure out what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flat_list = [item for sublist in problem[3] for item in sublist]\n",
    "# flat_list = [item for sublist in flat_list for item in sublist]\n",
    "# # flat_list[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check if we have a PAIR - otherwise we can't match\n",
    "\n",
    "# problem_x_indx = []\n",
    "# if len(problem) %2 == 0:\n",
    "#     for prblm in problem:\n",
    "#         flat_list = [item for sublist in prblm for item in sublist] # First level of flattening\n",
    "#         flat_list = [item for sublist in flat_list for item in sublist] # Flattens x and y into 1-D vector\n",
    "#         x_indices = flat_list[::2] # Every other element\n",
    "#         print(min(x_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edged[1,99:110] = 255\n",
    "# edged[-2, 185:196] = 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1a Key Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #####################################################################################\n",
    "# ####### using connected component detection algorithm to separate all the black notes\n",
    "# connectivity = 8\n",
    "# output = cv2.connectedComponentsWithStats(edged, connectivity, cv2.CV_32S)\n",
    "# num_labels = output[0]\n",
    "# labels = output[1]\n",
    "# stats = output[2]\n",
    "# centroids = output[3]\n",
    "\n",
    "# final_labels = []\n",
    "# note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "# output = img_notes_rgb.copy()\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# #For loop only used for displaying \n",
    "# for i in range(1, num_labels):\n",
    "#     x = stats[i, cv2.CC_STAT_LEFT]\n",
    "#     y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "#     w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "#     h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "#     area = stats[i, cv2.CC_STAT_AREA]\n",
    "#     (cX, cY) = centroids[i]\n",
    "#     cY = cY + 20 # We cropped out the first 20 pixels\n",
    "#     if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "#         final_labels.append(i)\n",
    "#         cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "#         dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "#         cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "#         componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "#         note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "#         note_list.append(note)\n",
    "        \n",
    "#         note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "#         cv2.putText(output, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "# #         displayImage(\"Output\", output)\n",
    "# #         displayImage(\"Connected Component\", componentMask)\n",
    "# #         cv2.waitKey(0)\n",
    "\n",
    "# # print(final_labels)\n",
    "# # cv2.destroyAllWindows()\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(output)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1b - Can just use connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "#Using standard threshold to create contrast between white/black keys\n",
    "_, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(th_notes, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####### using connected component detection algorithm to separate all the black notes\n",
    "connectivity = 8\n",
    "output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "num_labels = output[0]\n",
    "labels = output[1]\n",
    "stats = output[2]\n",
    "centroids = output[3]\n",
    "\n",
    "final_labels = []\n",
    "note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "output = img_notes_rgb.copy()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "#For loop only used for displaying \n",
    "for i in range(1, num_labels):\n",
    "    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "    area = stats[i, cv2.CC_STAT_AREA]\n",
    "    (cX, cY) = centroids[i]\n",
    "    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "    if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "        final_labels.append(i)\n",
    "        cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "        dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "        cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "        componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "        note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "        note_list.append(note)\n",
    "        \n",
    "        note_played, _ = getKeyPressed(keyboard.getKeys(), note.centroid_x)\n",
    "        cv2.putText(output, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Scenario Testing - Notes are VERY close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sample piano image from youtube\n",
    "# img_notes = cv2.imread(\"IMG_very_close_notes.png\")\n",
    "\n",
    "# #converting to gray\n",
    "# gray_notes = cv2.cvtColor(img_notes, cv2.COLOR_BGR2GRAY)\n",
    "# img_notes_rgb = cv2.cvtColor(img_notes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plt.imshow(img_notes_rgb)\n",
    "# # plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Cropped\n",
    "# top_keys_index = y_cord[1] + img_notes.shape[0]//2\n",
    "# crop_img_notes = img_notes[20:(int(top_keys_index)-30)] #Crop the top 20 pixels and bottom 30\n",
    "# crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# plt.imshow(crop_img_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to use connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "# blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "# #Using standard threshold to create contrast between white/black keys\n",
    "# _, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(th_notes, cmap = \"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #####################################################################################\n",
    "# ####### using connected component detection algorithm to separate all the black notes\n",
    "# connectivity = 8\n",
    "# output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "# num_labels = output[0]\n",
    "# labels = output[1]\n",
    "# stats = output[2]\n",
    "# centroids = output[3]\n",
    "\n",
    "# final_labels = []\n",
    "# note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "# output = img_notes_rgb.copy()\n",
    "# font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# #For loop only used for displaying \n",
    "# for i in range(1, num_labels):\n",
    "#     x = stats[i, cv2.CC_STAT_LEFT]\n",
    "#     y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "#     w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "#     h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "#     area = stats[i, cv2.CC_STAT_AREA]\n",
    "#     (cX, cY) = centroids[i]\n",
    "#     cY = cY + 20 # We cropped out the first 20 pixels\n",
    "#     if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "#         final_labels.append(i)\n",
    "#         cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "#         dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "#         cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "#         componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "#         note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "#         note_list.append(note)\n",
    "        \n",
    "#         note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "#         cv2.putText(output, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(output)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Scenario Testing - Notes ARE connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample piano image from youtube\n",
    "img_notes = cv2.imread(root + \"images\\IMG_connected_white_black.png\")\n",
    "\n",
    "#converting to gray\n",
    "gray_notes = cv2.cvtColor(img_notes, cv2.COLOR_BGR2GRAY)\n",
    "img_notes_rgb = cv2.cvtColor(img_notes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_notes_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropped\n",
    "top_keys_index = y_cord[1] + img_notes.shape[0]//2\n",
    "crop_img_notes = img_notes[20:(int(top_keys_index)-30)] #Crop the top 20 pixels and bottom 30\n",
    "crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(crop_img_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "#Using standard threshold to create contrast between white/black keys\n",
    "_, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(th_notes, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3a - Use Connected Components\n",
    "Then figure out if any region has a component that is much larger than the white note width.\n",
    "\n",
    "If yes, apply a mask on this and then use Canny-edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####### using connected component detection algorithm to separate all the black notes\n",
    "connectivity = 8\n",
    "output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "num_labels = output[0]\n",
    "labels = output[1]\n",
    "stats = output[2]\n",
    "centroids = output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all the connected components\n",
    "for i in range(1, num_labels):\n",
    "    \n",
    "    curr_connected_w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "    \n",
    "    #Determine if the WIDTH is much bigger than the width of a white key\n",
    "    if curr_connected_w > key_width_w*1.1:\n",
    "        print(i)\n",
    "        componentMask = (labels == i).astype(\"uint8\") * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# componentMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshMask = cv2.bitwise_and(crop_img_notes_gray, crop_img_notes_gray, mask = componentMask)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(threshMask, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3b Histogram\n",
    "Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Histogram\n",
    "#cv2.calcHist(images,channels,mask,histSize,ranges)\n",
    "bin_scaler = 4\n",
    "\n",
    "hist = cv2.calcHist([threshMask], [0], None, [int(256/4)], [1, 256])\n",
    "    #Grayscale has one channel so we use [0]\n",
    "    #No mask,\n",
    "    #We'll use 256 bins (i.e. count each and every - no binning)\n",
    "    #Possible values range from 0 to 256\n",
    "\n",
    "plt.title(\"Grayscale Histogram\")\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"# of Pixels\")\n",
    "plt.plot(hist)\n",
    "# plt.xlim([0, 256])\n",
    "# plt.figure(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colour thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = hist.argmax() * bin_scaler * .9\n",
    "th2 = threshMask.copy()\n",
    "th2[th2>T] = 255\n",
    "th2[th2<T] = 0\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(th2, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect the first set of keys (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####### using connected component detection algorithm to separate all the black notes\n",
    "connectivity = 8\n",
    "output = cv2.connectedComponentsWithStats(th2, connectivity, cv2.CV_32S)\n",
    "num_labels = output[0]\n",
    "labels = output[1]\n",
    "stats = output[2]\n",
    "centroids = output[3]\n",
    "\n",
    "final_labels = []\n",
    "note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "output = img_notes_rgb.copy()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "#For loop only used for displaying \n",
    "for i in range(1, num_labels):\n",
    "    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "    area = stats[i, cv2.CC_STAT_AREA]\n",
    "    (cX, cY) = centroids[i]\n",
    "    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "    if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "        final_labels.append(i)\n",
    "        cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "        dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "        cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "        componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "        note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "        note_list.append(note)\n",
    "        \n",
    "        note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "        cv2.putText(output, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the second set of keys (black) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = threshMask.copy()\n",
    "tst[tst>T] = 0\n",
    "\n",
    "plt.imshow(tst, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "blurred = cv2.GaussianBlur(tst, (k,k), 0)\n",
    "\n",
    "#Using standard threshold to create contrast between white/black keys\n",
    "_, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(th_notes, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####### using connected component detection algorithm to separate all the black notes\n",
    "connectivity = 8\n",
    "output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "num_labels = output[0]\n",
    "labels = output[1]\n",
    "stats = output[2]\n",
    "centroids = output[3]\n",
    "\n",
    "final_labels = []\n",
    "note_list = [] #creating a list of all the relavent notes. \n",
    "\n",
    "\n",
    "output = img_notes_rgb.copy()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "#For loop only used for displaying \n",
    "for i in range(1, num_labels):\n",
    "    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "    area = stats[i, cv2.CC_STAT_AREA]\n",
    "    (cX, cY) = centroids[i]\n",
    "    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "    if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "        final_labels.append(i)\n",
    "        cv2.rectangle(output, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "        dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "        cv2.circle(output, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "        componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        \n",
    "        note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "        note_list.append(note)\n",
    "        \n",
    "        note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "        cv2.putText(output, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Putting it Together\n",
    "First, utilize ConnectedComponents to determine our keys. If the width of a connect component is ~10% greater than the defined median width of a white key, we assume that these keys needs to be split.\n",
    "\n",
    "The only possibly connection would be a black key and a white key because black-black will never be fully connected nor will white-white. Thus, we can utilize thresholds to segment each connected key.\n",
    "\n",
    "With this, we remove the original connected components and then append the new separated keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample piano image from youtube\n",
    "img_notes = cv2.imread(root + \"images/IMG_connected_white_black.png\")\n",
    "\n",
    "#converting to gray\n",
    "gray_notes = cv2.cvtColor(img_notes, cv2.COLOR_BGR2GRAY)\n",
    "img_notes_rgb = cv2.cvtColor(img_notes, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_notes_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cropped\n",
    "top_keys_index = y_cord[1] + img_notes.shape[0]//2\n",
    "crop_img_notes = img_notes[20:(int(top_keys_index)-30)] #Crop the top 20 pixels and bottom 30\n",
    "crop_img_notes_gray = cv2.cvtColor(crop_img_notes, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(crop_img_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "blurred = cv2.GaussianBlur(crop_img_notes_gray, (k,k), 0)\n",
    "\n",
    "#Using standard threshold to create contrast between white/black keys\n",
    "_, th_notes = cv2.threshold(blurred, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(th_notes, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We have a thresholded image for use to use ConnectedComponents on\n",
    "\n",
    "#####################################################################################\n",
    "####### using connected component detection algorithm to separate all the black notes\n",
    "connectivity = 8\n",
    "output = cv2.connectedComponentsWithStats(th_notes, connectivity, cv2.CV_32S)\n",
    "num_labels = output[0]\n",
    "labels = output[1]\n",
    "stats = output[2]\n",
    "centroids = output[3]\n",
    "\n",
    "indices_to_pop = []\n",
    "\n",
    "\n",
    "output_img = img_notes_rgb.copy()\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "note_list = []\n",
    "\n",
    "i=1\n",
    "#Loop through all the connected components\n",
    "while i < len(stats):\n",
    "    \n",
    "    curr_connected_w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "    \n",
    "    #Determine if the WIDTH is much bigger than the width of a white key\n",
    "    if curr_connected_w > key_width_w*1.1:\n",
    "        \n",
    "        #Threshold just the large component of interest\n",
    "        componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "        threshMask = cv2.bitwise_and(crop_img_notes_gray, crop_img_notes_gray, mask = componentMask) #Replace this with video frame\n",
    "\n",
    "        # Histogram segregation of black/white key\n",
    "        # Grayscale has one channel so we use [0]\n",
    "            #Possible values range from 0 to 256\n",
    "        bin_scaler = 4\n",
    "        hist = cv2.calcHist([threshMask], [0], None, [256/bin_scaler], [1, 256])\n",
    "            \n",
    "\n",
    "        #Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image.\n",
    "        T = hist.argmax() * bin_scaler * .9\n",
    "        th1 = threshMask.copy()\n",
    "        th1[th1>T] = 255\n",
    "        th1[th1<T] = 0\n",
    "\n",
    "        #Detect the first set of keys\n",
    "        connectivity = 8\n",
    "        output = cv2.connectedComponentsWithStats(th1, connectivity, cv2.CV_32S)\n",
    "        num_labels_th1 = output[0]\n",
    "        labels_th1 = output[1]\n",
    "        stats_th1 = output[2]\n",
    "        centroids_th1 = output[3]\n",
    "        \n",
    "        #Loop through components and determine which ones may be keys\n",
    "        for j in range(1, num_labels_th1):\n",
    "            area = stats_th1[j, cv2.CC_STAT_AREA]\n",
    "            if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "                \n",
    "                if j > 1:\n",
    "                    ## We've added another label\n",
    "                    num_labels +=1 \n",
    "                    i +=1\n",
    "                \n",
    "                ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                #First, cut out the original \"fat\" label\n",
    "                fat_mask = labels != i\n",
    "                labels = labels * fat_mask\n",
    "                \n",
    "                #Next, increment each label above the cut one up to accomodate the new label\n",
    "                higher_mask = labels > i\n",
    "                labels = labels + higher_mask\n",
    "                \n",
    "                #Then append our segregated key\n",
    "                new_mask = labels_th1 == j\n",
    "                new_labels = labels_th1 * new_mask\n",
    "                new_labels = i * new_labels\n",
    "                labels = labels + new_labels\n",
    "                \n",
    "                ##Remove the original index for the stats and then add the new one\n",
    "                stats = np.delete(stats,i,0)\n",
    "                stats = np.insert(stats,i,stats_th1[j],0)\n",
    "                \n",
    "                ##Remove the original index for the centroids and then add the new one\n",
    "                centroids = np.delete(centroids,i,0)\n",
    "                centroids = np.insert(centroids,i,centroids_th1[j],0)\n",
    "                \n",
    "                #Plot immediately so indexing doesn't get messed up\n",
    "                x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                area = stats[i, cv2.CC_STAT_AREA]\n",
    "                (cX, cY) = centroids[i]\n",
    "                cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                cv2.circle(output_img, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "\n",
    "                note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "                note_list.append(note)\n",
    "\n",
    "                note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "                cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "            #Detect the next set of keys\n",
    "            th2 = threshMask.copy()\n",
    "            th2[th2>T] = 0        \n",
    "            k = 3\n",
    "            blurred_th2 = cv2.GaussianBlur(th2, (k,k), 0)\n",
    "\n",
    "            #Using standard threshold to create contrast between white/black keys\n",
    "            _, th2_notes = cv2.threshold(blurred_th2, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "            #Detect the second set of keys\n",
    "            connectivity = 8\n",
    "            output = cv2.connectedComponentsWithStats(th2_notes, connectivity, cv2.CV_32S)\n",
    "            num_labels_th2 = output[0]\n",
    "            labels_th2 = output[1]\n",
    "            stats_th2 = output[2]\n",
    "            centroids_th2 = output[3]\n",
    "\n",
    "            #Loop through components and determine which ones may be keys\n",
    "            for k in range(1, num_labels_th2):\n",
    "                area = stats_th2[k, cv2.CC_STAT_AREA]\n",
    "                if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                    if k > 1:\n",
    "                        ## We've added another label\n",
    "                        num_labels +=1 \n",
    "                        i+=1\n",
    "\n",
    "                    ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                    #For the second key WE DON'T NEED TO CUT anything\n",
    "    #                 fat_mask = labels != i\n",
    "    #                 labels = labels * fat_mask\n",
    "\n",
    "                    #Next, increment each label above the cut one up to accomodate the new label\n",
    "                    higher_mask = labels > i + 1\n",
    "                    labels = labels + higher_mask\n",
    "\n",
    "                    #Then append our segregated key\n",
    "                    new_mask = labels_th2 == k\n",
    "                    new_labels = labels_th2 * new_mask\n",
    "                    new_labels = (i + 1) * new_labels\n",
    "                    labels = labels + new_labels\n",
    "\n",
    "                    ##Remove the original index for the stats and then add the new one\n",
    "    #                 stats = np.delete(stats,(i+1),0)\n",
    "                    stats = np.insert(stats,(i+1),stats_th2[k],0)\n",
    "\n",
    "                    ##Remove the original index for the centroids and then add the new one\n",
    "    #                 centroids = np.delete(centroids,(i+1),0)\n",
    "                    centroids = np.insert(centroids,(i+1),centroids_th2[k],0)\n",
    "        \n",
    "                    #Plot immediately so indexing doesn't get messed up\n",
    "                    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                    area = stats[i, cv2.CC_STAT_AREA]\n",
    "                    (cX, cY) = centroids[i]\n",
    "                    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                    cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "                    dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                    cv2.circle(output_img, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "\n",
    "                    note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "                    note_list.append(note)\n",
    "\n",
    "                    note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "                    cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "    else:\n",
    "        x = stats[i, cv2.CC_STAT_LEFT]\n",
    "        y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "        w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "        h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        (cX, cY) = centroids[i]\n",
    "        cY = cY + 20 # We cropped out the first 20 pixels\n",
    "        if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "            cv2.rectangle(output_img, (x,y), (x+w, y+h), (255,0,0),1)\n",
    "            dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "            cv2.circle(output_img, (int(cX), int(cY+dist_to_edge)), 1, (0,122,255), 3)\n",
    "\n",
    "            note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            note_list.append(note)\n",
    "\n",
    "            note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "            cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(output_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Do it on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(root + 'videos/video_to_process.mp4')\n",
    "\n",
    "frames = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = camera.get(cv2.CAP_PROP_FPS)\n",
    "seconds_per_frame = fps/frames\n",
    "counter = 0\n",
    "\n",
    "notes_pressed = []\n",
    "\n",
    "keys_timed = []\n",
    "for x in full_key_list:\n",
    "    keys_timed.append([x[0]])\n",
    "\n",
    "\n",
    "keys_timed_update = []\n",
    "for x in full_key_list:\n",
    "    keys_timed_update.append([x[0]])\n",
    "\n",
    "    \n",
    "testing_screen = []\n",
    "testing_mask = []\n",
    "\n",
    "while (camera.isOpened()):\n",
    "    #print(counter)\n",
    "    #print('-'*100)\n",
    "    \n",
    "    #grabbed is a boolean than tells us if there is a valid frame\n",
    "    (grabbed, frame) = camera.read()\n",
    "\n",
    "    frame_number = camera.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    elapsed = frame_number/fps\n",
    "\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    counter += seconds_per_frame\n",
    "        \n",
    "         \n",
    "    frame = imutils.resize(frame,width = keys.shape[1]) #resize or else it won't work\n",
    "    #print(frame.shape)\n",
    "    \n",
    "    crop_frame = frame[20:int(top_keys_index)-50] #Crop the top 20 pixels and bottom 10\n",
    "    \n",
    "    # threshold the cropped and grayed image\n",
    "    crop_frame_gray = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, th_crop_frame = cv2.threshold(crop_frame_gray, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "    #We have a thresholded image for use to use ConnectedComponents on\n",
    "    #####################################################################################\n",
    "    ####### using connected component detection algorithm to separate all the black notes\n",
    "    connectivity = 8\n",
    "    output = cv2.connectedComponentsWithStats(th_crop_frame, connectivity, cv2.CV_32S)\n",
    "    num_labels = output[0]\n",
    "    labels = output[1]\n",
    "    stats = output[2]\n",
    "    centroids = output[3]\n",
    "\n",
    "    indices_to_pop = []\n",
    "\n",
    "\n",
    "    output_img = frame.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    \n",
    "#     print(\"-\"*50)\n",
    "#     print(\"new frame\")\n",
    "    i=1\n",
    "    #Loop through all the connected components\n",
    "    while i < len(stats):\n",
    "\n",
    "        curr_connected_w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "\n",
    "        #Determine if the WIDTH is much bigger than the width of a white key and less than 4x? (So we don't get extraneous video text, etc.)\n",
    "        if curr_connected_w > key_width_w*1.25 and curr_connected_w < key_width_w*4:\n",
    "            \n",
    "\n",
    "\n",
    "            #Threshold just the large component of interest\n",
    "            componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "            threshMask = cv2.bitwise_and(crop_frame_gray, crop_frame_gray, mask = componentMask) #Replace this with video frame\n",
    "\n",
    "#             print(\"We are here.\")\n",
    "#             testing_screen.append(frame.copy())\n",
    "#             testing_mask.append(threshMask.copy())\n",
    "            \n",
    "            # Histogram segregation of black/white key\n",
    "            # Grayscale has one channel so we use [0]\n",
    "                #Possible values range from 0 to 256\n",
    "            bin_scaler = 4\n",
    "            hist = cv2.calcHist([threshMask], [0], None, [int(256/bin_scaler)], [1, 256])\n",
    "\n",
    "\n",
    "            #Use a Histogram to compute the dominant non-black (i.e. not the background) colour. Use ~90% of this to threshold the image.\n",
    "            T = hist.argmax() * bin_scaler * .9\n",
    "            th1 = threshMask.copy()\n",
    "            th1[th1>T] = 255\n",
    "            th1[th1<T] = 0\n",
    "\n",
    "            #Detect the first set of keys\n",
    "            connectivity = 8\n",
    "            output = cv2.connectedComponentsWithStats(th1, connectivity, cv2.CV_32S)\n",
    "            num_labels_th1 = output[0]\n",
    "            labels_th1 = output[1]\n",
    "            stats_th1 = output[2]\n",
    "            centroids_th1 = output[3]\n",
    "\n",
    "            #Loop through components and determine which ones may be keys\n",
    "            for j in range(1, num_labels_th1):\n",
    "                area = stats_th1[j, cv2.CC_STAT_AREA]\n",
    "                if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                    if j > 1:\n",
    "                        ## We've added another label\n",
    "                        num_labels +=1 \n",
    "                        i +=1\n",
    "\n",
    "                    ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                    #First, cut out the original \"fat\" label\n",
    "                    fat_mask = labels != i\n",
    "                    labels = labels * fat_mask\n",
    "\n",
    "                    #Next, increment each label above the cut one up to accomodate the new label\n",
    "                    higher_mask = labels > i\n",
    "                    labels = labels + higher_mask\n",
    "\n",
    "                    #Then append our segregated key\n",
    "                    new_mask = labels_th1 == j\n",
    "                    new_labels = labels_th1 * new_mask\n",
    "                    new_labels = i * new_labels\n",
    "                    labels = labels + new_labels\n",
    "\n",
    "                    ##Remove the original index for the stats and then add the new one\n",
    "                    if i < len(stats):\n",
    "                        stats = np.delete(stats,i,0)\n",
    "                        stats = np.insert(stats,i,stats_th1[j],0)\n",
    "                    elif j == 1:\n",
    "                        stats = stats[:-1,:]\n",
    "                        stats = np.concatenate((stats,stats_th1[j][None,:]),0)\n",
    "                    else:\n",
    "                        stats = np.concatenate((stats,stats_th1[j][None,:]),0)\n",
    "\n",
    "                    ##Remove the original index for the centroids and then add the new one\n",
    "                    if i < len(centroids):\n",
    "                        centroids = np.delete(centroids,i,0)\n",
    "                        centroids = np.insert(centroids,i,centroids_th1[j],0)\n",
    "                    elif j==1:\n",
    "                        centroids = centroids[:-1,:]\n",
    "                        centroids = np.concatenate((centroids,centroids_th1[j][None,:]),0)  \n",
    "                    else:\n",
    "                        centroids = np.concatenate((centroids,centroids_th1[j][None,:]),0)  \n",
    "                        \n",
    "\n",
    "                        \n",
    "                    print(\"i: {}, stats: {}\".format(i,len(stats)))\n",
    "                    #Plot immediately so indexing doesn't get messed up\n",
    "                    x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                    y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                    w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                    h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                    area = stats[i, cv2.CC_STAT_AREA]\n",
    "                    (cX, cY) = centroids[i]\n",
    "                    cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                    cv2.rectangle(output_img, (x,y), (x+w, y+h), (0,0,255),1)\n",
    "                    dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                    top_dot = cY-dist_to_edge\n",
    "                    bottom_dot = cY+dist_to_edge\n",
    "                    cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                    cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                    note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "            \n",
    "                    if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                        note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                        #print(str(note_played) + \" at \" + str(counter))\n",
    "                        keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                    if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                        note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                        keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                    note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "                    cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #Detect the next set of keys\n",
    "                th2 = threshMask.copy()\n",
    "                th2[th2>T] = 0        \n",
    "                k = 3\n",
    "                blurred_th2 = cv2.GaussianBlur(th2, (k,k), 0)\n",
    "\n",
    "                #Using standard threshold to create contrast between white/black keys\n",
    "                _, th2_notes = cv2.threshold(blurred_th2, 90, 150, cv2.THRESH_BINARY)\n",
    "\n",
    "                #Detect the second set of keys\n",
    "                connectivity = 8\n",
    "                output = cv2.connectedComponentsWithStats(th2_notes, connectivity, cv2.CV_32S)\n",
    "                num_labels_th2 = output[0]\n",
    "                labels_th2 = output[1]\n",
    "                stats_th2 = output[2]\n",
    "                centroids_th2 = output[3]\n",
    "\n",
    "                #Loop through components and determine which ones may be keys\n",
    "                for k in range(1, num_labels_th2):\n",
    "                    area = stats_th2[k, cv2.CC_STAT_AREA]\n",
    "                    if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be keys)\n",
    "\n",
    "                        if k > 1:\n",
    "                            ## We've added another label\n",
    "                            num_labels +=1 \n",
    "                            i+=1\n",
    "\n",
    "                        ##Within labels_th1, we have a matrix that is the same size of the image that holds our split component\n",
    "                        #For the second key WE DON'T NEED TO CUT anything\n",
    "        #                 fat_mask = labels != i\n",
    "        #                 labels = labels * fat_mask\n",
    "\n",
    "                        #Next, increment each label above the cut one up to accomodate the new label\n",
    "                        higher_mask = labels > i + 1\n",
    "                        labels = labels + higher_mask\n",
    "\n",
    "                        #Then append our segregated key\n",
    "                        new_mask = labels_th2 == k\n",
    "                        new_labels = labels_th2 * new_mask\n",
    "                        new_labels = (i + 1) * new_labels\n",
    "                        labels = labels + new_labels\n",
    "\n",
    "                        ##Add\n",
    "                        if i < len(stats):\n",
    "                            stats = np.insert(stats,(i+1),stats_th2[k],0)                       \n",
    "                        else:\n",
    "                            stats = np.concatenate((stats,stats_th2[k][None,:]),0)\n",
    "                        \n",
    "\n",
    "                        ##Add\n",
    "                        if i < len(centroids):\n",
    "                            centroids = np.insert(centroids,(i+1),centroids_th2[k],0)\n",
    "                        else:\n",
    "                            centroids = np.concatenate((centroids,centroids_th2[k][None,:]),0)\n",
    "\n",
    "                        #Plot immediately so indexing doesn't get messed up\n",
    "                        x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                        y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "                        w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                        h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "                        area = stats[i, cv2.CC_STAT_AREA]\n",
    "                        (cX, cY) = centroids[i]\n",
    "                        cY = cY + 20 # We cropped out the first 20 pixels\n",
    "                        cv2.rectangle(output_img, (x,y), (x+w, y+h), (0,0,255),1)\n",
    "                        dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                        top_dot = cY-dist_to_edge\n",
    "                        bottom_dot = cY+dist_to_edge\n",
    "                        cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                        cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                        note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "            \n",
    "                        if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                            note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                            #print(str(note_played) + \" at \" + str(counter))\n",
    "                            keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                        if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                            note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                            keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                        note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "                        cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "        else:\n",
    "            x = stats[i, cv2.CC_STAT_LEFT]\n",
    "            y = stats[i, cv2.CC_STAT_TOP] + 20 # We cropped out the first 20 pixels\n",
    "            w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            (cX, cY) = centroids[i]\n",
    "            cY = cY + 20 # We cropped out the first 20 pixels\n",
    "            if (20 < area < np.inf): #filtering out relavent detections (the ones big enough to be black keys)\n",
    "                cv2.rectangle(output_img, (x,y), (x+w, y+h), (0,0,255),2)\n",
    "                dist_to_edge = h/2 #getting the distance from centroid to bottom edge for better detection later on\n",
    "                top_dot = cY-dist_to_edge\n",
    "                bottom_dot = cY+dist_to_edge\n",
    "                cv2.circle(output_img, (int(cX), int(bottom_dot)), 1, (0,122,255), 3)\n",
    "                cv2.circle(output_img, (int(cX), int(top_dot)), 1, (0,122,255), 3)\n",
    "\n",
    "                note = Note(cX, cY+dist_to_edge) #creating note object and adding to list\n",
    "            \n",
    "                if ( (int(bottom_dot) >= int(y_cord[0])) and (int(bottom_dot) <= int(y_cord[0])+2) ):\n",
    "                    note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                    #print(str(note_played) + \" at \" + str(counter))\n",
    "                    keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                if ( (int(top_dot) >= int(y_cord[0])) and (int(top_dot) <= int(y_cord[0])+2) ):\n",
    "                    note_played, index = getKeyPressed(full_key_list, cX)\n",
    "                    keys_timed_update[index].append([elapsed])\n",
    "\n",
    "                note_played, _ = getKeyPressed(full_key_list, note.centroid_x)\n",
    "                cv2.putText(output_img, note_played, (int(note.centroid_x), int(note.centroid_y)), font, 0.5, (0,255,0), 1)\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    #Show the frame + drawn rectangle\n",
    "    cv2.imshow(\"Video\", output_img)\n",
    "\n",
    "    #Can break early by pressing \"q\"\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "# print(keys_timed_update)    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
